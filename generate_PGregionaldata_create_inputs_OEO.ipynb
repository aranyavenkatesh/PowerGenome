{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from powergenome.generators import GeneratorClusters, add_genx_model_tags\n",
    "from powergenome.GenX import reduce_time_domain\n",
    "from powergenome.load_profiles import make_final_load_curves\n",
    "from powergenome.params import DATA_PATHS\n",
    "from powergenome.util import (\n",
    "    build_scenario_settings,\n",
    "    init_pudl_connection,\n",
    "    load_settings,\n",
    ")\n",
    "from powergenome.external_data import (\n",
    "    make_demand_response_profiles,\n",
    "    make_generator_variability,\n",
    ")\n",
    "\n",
    "from powergenome.load_profiles import (\n",
    "    make_load_curves, \n",
    "    add_load_growth, \n",
    "    make_final_load_curves, \n",
    "    make_distributed_gen_profiles,\n",
    ")\n",
    "from powergenome.external_data import make_demand_response_profiles\n",
    "from powergenome.generators import GeneratorClusters\n",
    "from powergenome.util import (\n",
    "    build_scenario_settings,\n",
    "    init_pudl_connection,\n",
    "    load_settings,\n",
    "    reverse_dict_of_lists,\n",
    "    remove_feb_29\n",
    ")\n",
    "\n",
    "from powergenome.load_profiles import make_final_load_curves\n",
    "from powergenome.generators import GeneratorClusters\n",
    "from powergenome.util import (\n",
    "    build_scenario_settings,\n",
    "    init_pudl_connection,\n",
    "    load_settings,\n",
    "    reverse_dict_of_lists\n",
    ")\n",
    "\n",
    "from powergenome.GenX import reduce_time_domain, add_misc_gen_values\n",
    "from powergenome.external_data import make_generator_variability\n",
    "\n",
    "from powergenome.generators import load_ipm_shapefile\n",
    "from powergenome.GenX import (\n",
    "    network_line_loss,\n",
    "    network_max_reinforcement,\n",
    "    network_reinforcement_cost,\n",
    ")\n",
    "from powergenome.transmission import (\n",
    "    agg_transmission_constraints,\n",
    "    transmission_line_distance,\n",
    ")\n",
    "from powergenome.util import init_pudl_connection, load_settings\n",
    "\n",
    "pd.options.display.max_columns = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sqlite3\n",
    "import shutil\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_region = 0\n",
    "if single_region ==1:\n",
    "    run_folder = \"US_Regional\" \n",
    "    settings_file = \"Temoa_settings_national.yml\" \n",
    "    scenario = \"p6\"\n",
    "else:\n",
    "    run_folder = \"US_Regional\"\n",
    "    settings_file = \"Temoa_settings.yml\"\n",
    "    scenario = \"p6\"\n",
    "timeslice = 0\n",
    "elec_only = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create New SQL File\n",
    "emptydB = '/Users/aranyavenkatesh/Documents/EnergyOutlook/temoa/data_files/US_National.sqlite'\n",
    "#outfilename_w_ext = outFilename + '.sqlite'\n",
    "if (single_region==1) & (timeslice==1) :\n",
    "    outputdB = '/Users/aranyavenkatesh/Documents/EnergyOutlook/temoa/data_files/US_National_rev.sqlite'\n",
    "elif (single_region==1) & (timeslice==0) :\n",
    "    outputdB = '/Users/aranyavenkatesh/Documents/EnergyOutlook/temoa/data_files/US_National_4.sqlite'\n",
    "elif elec_only ==1:\n",
    "    outputdB = '/Users/aranyavenkatesh/Documents/EnergyOutlook/temoa/data_files/US_Regional_elec.sqlite'\n",
    "else:\n",
    "    outputdB = '/Users/aranyavenkatesh/Documents/EnergyOutlook/temoa/data_files/US_Regional.sqlite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pudl_engine, pudl_out = init_pudl_connection()\n",
    "cwd = Path.cwd()\n",
    "\n",
    "settings_path = (\n",
    "    cwd / run_folder / settings_file\n",
    ")\n",
    "settings = load_settings(settings_path)\n",
    "settings[\"input_folder\"] = settings_path.parent / settings[\"input_folder\"]\n",
    "scenario_definitions = pd.read_csv(\n",
    "    settings[\"input_folder\"] / settings[\"scenario_definitions_fn\"]\n",
    ")\n",
    "scenario_settings = build_scenario_settings(settings, scenario_definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_periods = list(scenario_settings.keys())\n",
    "start_year = all_periods[0]\n",
    "run_new = 0\n",
    "file_prefix = str(settings_path).replace('.yml','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if run_new==1:\n",
    "    new_gen = pd.DataFrame()\n",
    "    for year in all_periods[1:]: #new gen for start year is included in all_gen\n",
    "        gc = GeneratorClusters(pudl_engine, pudl_out, scenario_settings[year][scenario])\n",
    "        new_gen_year = gc.create_new_generators()\n",
    "        new_gen_year.loc[:,'operating_year'] = year\n",
    "        new_gen = pd.concat([new_gen, new_gen_year]) #create new generators for periods beyond the first\n",
    "        \n",
    "    load_curves = make_final_load_curves(pudl_engine, scenario_settings[start_year][scenario])\n",
    "\n",
    "    gc = GeneratorClusters(pudl_engine, pudl_out, scenario_settings[start_year][scenario])\n",
    "    all_gens = gc.create_all_generators() #create existing and new generators for the first time period\n",
    "\n",
    "    #add misc_values from misc_gen_inputs_fn file in extra_inputs folder\n",
    "    all_gens = add_misc_gen_values(all_gens,settings)\n",
    "    new_gen = add_misc_gen_values(new_gen,settings)\n",
    "\n",
    "    gen_variability = make_generator_variability(all_gens)\n",
    "\n",
    "    (\n",
    "        reduced_resource_profile,\n",
    "        reduced_load_profile,\n",
    "        long_duration_storage,\n",
    "    ) = reduce_time_domain(gen_variability, load_curves, scenario_settings[start_year][scenario])\n",
    "\n",
    "    if len(settings['region_aggregations'])>1:\n",
    "        transmission = agg_transmission_constraints(pudl_engine=pudl_engine, settings=settings)\n",
    "        model_regions_gdf = load_ipm_shapefile(settings)\n",
    "        transmission = transmission_line_distance(\n",
    "            trans_constraints_df=transmission,\n",
    "            ipm_shapefile=model_regions_gdf,\n",
    "            settings=settings,\n",
    "        )\n",
    "        transmission = network_line_loss(transmission=transmission, settings=settings)\n",
    "        transmission = network_reinforcement_cost(transmission=transmission, settings=settings)\n",
    "        transmission = network_max_reinforcement(transmission=transmission, settings=settings)\n",
    "        transmission.to_csv(file_prefix + 'transmission.csv', index=False)\n",
    "\n",
    "    gen_variability.to_csv(file_prefix + 'gen_variability.csv', index=False)\n",
    "    load_curves.to_csv(file_prefix + 'load_curves.csv', index=False)\n",
    "    reduced_load_profile.to_csv(file_prefix + 'reduced_load_profile.csv', index=False)\n",
    "    reduced_resource_profile.to_csv(file_prefix + 'reduced_resource_profile.csv', index=False)\n",
    "    all_gens.to_csv(file_prefix + 'all_gens.csv', index=False)\n",
    "    new_gen.to_csv(file_prefix + 'new_gen.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if elec_only==1:\n",
    "    elec_demand = pd.DataFrame()\n",
    "    for year in all_periods:\n",
    "        load_curves_year = make_final_load_curves(pudl_engine, scenario_settings[year][scenario])\n",
    "        load_curves_year.loc[:,'periods']=year\n",
    "        elec_demand = pd.concat([elec_demand, load_curves_year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_lifetimes = dict()\n",
    "generator_lifetimes['BECCS'] = 40\n",
    "generator_lifetimes['BIO'] = 40\n",
    "generator_lifetimes['Batt'] = 40\n",
    "generator_lifetimes['COALIGCC'] = 40\n",
    "generator_lifetimes['COALSTM'] = 60\n",
    "generator_lifetimes['COALUSC'] = 40\n",
    "generator_lifetimes['GEO'] = 100\n",
    "generator_lifetimes['H2'] = 40\n",
    "generator_lifetimes['HYD'] = 500\n",
    "generator_lifetimes['NGA'] = 60\n",
    "generator_lifetimes['WND'] = 30\n",
    "generator_lifetimes['SOL'] = 30\n",
    "generator_lifetimes['TRANS'] = 30\n",
    "generator_lifetimes['URN'] = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_new!=1:\n",
    "    #reduced_load_profile = pd.read_csv(file_prefix + 'reduced_load_profile.csv')\n",
    "    #reduced_resource_profile = pd.read_csv(file_prefix + 'reduced_resource_profile.csv')\n",
    "    all_gens = pd.read_csv(file_prefix + 'all_gens.csv')\n",
    "    new_gen = pd.read_csv(file_prefix + 'new_gen.csv')\n",
    "    \n",
    "    gen_variability = pd.read_csv(file_prefix + 'gen_variability.csv')\n",
    "    load_curves = pd.read_csv(file_prefix + 'load_curves.csv')\n",
    "    \n",
    "    \n",
    "    (\n",
    "        reduced_resource_profile,\n",
    "        reduced_load_profile,\n",
    "        long_duration_storage,\n",
    "    ) = reduce_time_domain(gen_variability, load_curves, scenario_settings[start_year][scenario])\n",
    "    \n",
    "    if len(settings['region_aggregations'])>1:\n",
    "        transmission = pd.read_csv(file_prefix + 'transmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_tech_names = dict()\n",
    "map_tech_names['biomass'] = 'E_BIO_R'\n",
    "map_tech_names['conventional_hydroelectric'] = 'E_HYDCONV_R'\n",
    "map_tech_names['conventional_steam_coal'] = 'E_COALSTM_N'\n",
    "map_tech_names['natural_gas_fired_combined_cycle'] = 'E_NGACC_R'\n",
    "map_tech_names['natural_gas_fired_combustion_turbine'] = 'E_NGACT_R'\n",
    "map_tech_names['natural_gas_steam_turbine'] = 'E_NGASTM_R'\n",
    "map_tech_names['nuclear'] = 'E_URNLWR_R'\n",
    "map_tech_names['onshore_wind_turbine'] = 'E_WND_R'\n",
    "map_tech_names['small_hydroelectric'] = 'E_HYDSM_R'\n",
    "map_tech_names['solar_photovoltaic'] = 'E_SOLPV_R'\n",
    "map_tech_names['hydroelectric_pumped_storage'] = 'E_HYDPS_R'\n",
    "map_tech_names['geothermal'] = 'E_GEO_R'\n",
    "map_tech_names['naturalgas_ccccsavgcf_mid'] = 'E_NGACC_CCS_N'\n",
    "map_tech_names['naturalgas_ccavgcf_mid'] = 'E_NGAACC_N'\n",
    "map_tech_names['naturalgas_ctavgcf_mid'] = 'E_NGAACT_N'\n",
    "map_tech_names['landbasedwind_ltrg1_mid'] = 'E_WND_N'\n",
    "map_tech_names['utilitypv_losangeles_mid'] = 'E_SOLPVCEN_N'\n",
    "map_tech_names['naturalgas_ccs100_mid'] = 'E_NGACC_CCS_ZERO_N'\n",
    "map_tech_names['nuclear_mid'] = 'E_URNLWR_N'\n",
    "map_tech_names['battery_mid'] = 'E_Batt'\n",
    "map_tech_names['ev_load_shifting'] = 'E_LD_SHFT'\n",
    "map_tech_names['offshorewind_otrg10_mid'] = 'E_OFWND_N'\n",
    "map_tech_names['geothermal_hydroflash'] = 'E_GEOF_N'\n",
    "map_tech_names['geothermal_hydrobinary'] = 'E_GEOB_N'\n",
    "map_tech_names['csp_class5_10hourstes'] = 'E_SOLTHCEN_N'\n",
    "map_tech_names['biopower_dedicated_mid'] = 'E_BIO_N'\n",
    "map_tech_names['coal_igccavgcf_mid'] = 'E_COALIGCC_N'\n",
    "map_tech_names['coal_ccs90avgcf_mid'] = 'E_COALUSC_90CCS_N'\n",
    "map_tech_names['respv_losangeles'] = 'E_SOLPVENDUSE_N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_tech_desc = dict()\n",
    "map_tech_desc['E_BIO_R'] = '#existing bio-energy'\n",
    "map_tech_desc['E_BIO_N'] = '#new bio-energy'\n",
    "map_tech_desc['E_HYDSM_R'] = '#existing small hydroelectric power plant'\n",
    "map_tech_desc['E_HYDPS_R'] = '#existing pumped hydro storage'\n",
    "map_tech_desc['E_WND_N'] = '#new wind power plant'\n",
    "map_tech_desc['E_NGACC_CCS_ZERO_N'] = '#new natural gas combined cycle with 100% CCS power plant'\n",
    "map_tech_desc['E_OFWND_N'] = '#new offshore wind, floating'\n",
    "map_tech_desc['E_GEOF_N'] = '#new geothermal, hydro flash'\n",
    "map_tech_desc['E_GEOB_N'] = '#new geothermal, hydro binary'\n",
    "map_tech_desc['E_COALUSC_90CCS_N'] = '#new ultrasupercritical pulverized coal with 90% CCS power plant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_gens['Resource'] = all_gens['Resource'].str.split('_').map(lambda x: '_'.join(x[0:3] if x[-1].isnumeric() else x))\n",
    "#new_gen['Resource'] = new_gen['Resource'].str.split('_').map(lambda x: '_'.join(x[0:3] if x[-1].isnumeric() else x))\n",
    "#all_gens['Resource'].to_csv('resource.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map PG technology names to OEO names\n",
    "all_gens['Resource'] = all_gens['Resource'].str.split('_').map(lambda x: '_'.join(x[0:3] if x[-1].isnumeric() else x))\n",
    "new_gen['Resource'] = new_gen['Resource'].str.split('_').map(lambda x: '_'.join(x[0:3] if x[-1].isnumeric() else x))\n",
    "all_gens['Resource'] = all_gens['Resource'].map(map_tech_names)\n",
    "new_gen['Resource'] = new_gen['Resource'].map(map_tech_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiply capex values by the regional multipliers, since this is not done in PowerGenome\n",
    "new_gen['capex_mw'] *= new_gen['regional_cost_multiplier']\n",
    "all_gens['capex_mw'] *= all_gens['regional_cost_multiplier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(emptydB)\n",
    "c = conn.cursor()\n",
    "tech_table = pd.read_sql_query(\"SELECT * FROM technologies\", conn)\n",
    "tech_table.rename(columns={'tech':'tech_technologies'}, inplace=True)\n",
    "\n",
    "input_table = pd.read_sql_query(\"SELECT input_comm, tech FROM Efficiency WHERE tech in \\\n",
    "(SELECT tech FROM technologies where sector='electric')\", conn)\n",
    "input_table = {k: list(v) for k,v in input_table.drop_duplicates().groupby('tech')[\"input_comm\"]}\n",
    "conn.close()\n",
    "\n",
    "\n",
    "#input comms for technologies not specified in US_National\n",
    "input_table['E_BIO_R'] = ['AGR']\n",
    "input_table['E_BIO_N'] = input_table['E_BIOIGCC_N'] \n",
    "input_table['E_HYDPS_R'] = ['ethos_R']\n",
    "input_table['E_HYDSM_R'] = ['ethos_R']\n",
    "input_table['E_COALUSC_90CCS_N'] = ['COALIGCC_N'] #COALSTM_N\n",
    "input_table['E_COALIGCC_N'] = ['COALIGCC_N']#COALSTM_N\n",
    "input_table['E_NGACC_CCS_ZERO_N'] = ['E_NGA']\n",
    "input_table['E_GEOB_N'] = ['ethos_R']\n",
    "input_table['E_GEOF_N'] = ['ethos_R']\n",
    "input_table['E_WND_N'] = ['ethos_R']\n",
    "input_table['E_OFWND_N'] = ['ethos_R']\n",
    "input_table['E_TRANS_R'] = ['ELC']\n",
    "input_table['E_TRANS_N'] = ['ELC']\n",
    "\n",
    "#emissionsactivity for technologies not specified in US_National \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_count = all_gens[['technology', 'cluster']].groupby('technology').max().reset_index()\n",
    "cluster_count.columns = ['technology', 'max_cluster']\n",
    "all_gens = all_gens.merge(cluster_count, on='technology', how='left')\n",
    "\n",
    "mask = all_gens['max_cluster']<=1\n",
    "all_gens.loc[mask, 'tech'] =  all_gens.loc[mask, 'Resource'] + '-' + all_gens.loc[mask,'region']\n",
    "all_gens.loc[~mask, 'tech'] =  all_gens.loc[~mask, 'Resource'] + '-' + all_gens.loc[~mask,'region'] + \\\n",
    "'-' +all_gens.loc[~mask,'cluster'].map(int).map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_count = new_gen[['technology', 'cluster']].groupby('technology').max().reset_index()\n",
    "cluster_count.columns = ['technology', 'max_cluster']\n",
    "new_gen = new_gen.merge(cluster_count, on='technology', how='left')\n",
    "\n",
    "mask = new_gen['max_cluster']<=1\n",
    "new_gen.loc[mask, 'tech'] =  new_gen.loc[mask, 'Resource'] + '-' + new_gen.loc[mask,'region']\n",
    "new_gen.loc[~mask, 'tech'] =  new_gen.loc[~mask, 'Resource'] + '-' + new_gen.loc[~mask,'region'] + \\\n",
    "'-' +new_gen.loc[~mask,'cluster'].map(int).map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat start year gens with remaining new gens\n",
    "all_gens_multi_year = pd.concat([all_gens, new_gen])\n",
    "\n",
    "all_gens_multi_year = all_gens_multi_year.merge(tech_table, left_on = ['Resource'], right_on =['tech_technologies'], how='left')\n",
    "all_gens_multi_year['flag'].fillna('p', inplace=True)\n",
    "all_gens_multi_year['sector'].fillna('electric', inplace=True)\n",
    "all_gens_multi_year['tech_desc'].fillna(all_gens_multi_year['Resource'].map(map_tech_desc), inplace=True)\n",
    "\n",
    "all_gens_multi_year.loc[all_gens_multi_year['Resource']=='E_HYDPS_R','flag'] = 'ps'\n",
    "\n",
    "#all_gens_multi_year = all_gens_multi_year[all_gens.columns]\n",
    "all_gens_multi_year.loc[np.isnan(all_gens_multi_year.operating_year),'operating_year'] = all_periods[0]\n",
    "all_gens_multi_year.loc[all_gens_multi_year.operating_year==0,'operating_year'] = all_periods[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#account for storage (batteries) capex and FOM for a specific storage duration\n",
    "stor_dur = 4\n",
    "mask = all_gens_multi_year['tech'].str.contains('Batt')\n",
    "all_gens_multi_year.loc[mask, 'Heat_rate_MMBTU_per_MWh'] = 3.412/0.85 #assuming 85% efficiency\n",
    "orig_batt = all_gens_multi_year.loc[mask,:].copy()\n",
    "all_gens_multi_year.loc[mask,'capex_mw'] += stor_dur*all_gens_multi_year.loc[mask,'capex_mwh']\n",
    "all_gens_multi_year.loc[mask,'Fixed_OM_cost_per_MWyr'] += stor_dur*all_gens_multi_year.loc[mask,'Fixed_OM_cost_per_MWhyr']\n",
    "#all_gens_multi_year.loc[mask,'Fixed_OM_cost_per_MWyr'] = 2.5/100*all_gens_multi_year.loc[mask,'capex_mwh']\n",
    "stor_dur = 8\n",
    "orig_batt.loc[mask,'capex_mw'] += stor_dur*orig_batt.loc[mask,'capex_mwh']\n",
    "orig_batt.loc[mask,'Fixed_OM_cost_per_MWyr'] += stor_dur*orig_batt.loc[mask,'Fixed_OM_cost_per_MWhyr']\n",
    "#orig_batt.loc[mask,'Fixed_OM_cost_per_MWyr'] = 2.5/100*orig_batt.loc[mask,'capex_mwh']\n",
    "orig_batt['Resource'] = 'E_Batt8hr'\n",
    "orig_batt['tech'] = orig_batt['tech'].str.replace('E_Batt','E_Batt8hr')\n",
    "orig_batt['tech_technologies'] = orig_batt['tech_technologies'].str.replace('E_Batt','E_Batt8hr')\n",
    "orig_batt['tech_desc'] = orig_batt['tech_desc'].str.replace('storage','storage (8 hour)')\n",
    "\n",
    "all_gens_multi_year = pd.concat([all_gens_multi_year, orig_batt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicate NGCC costs, efficiencies to represent H2CC\n",
    "mask = all_gens_multi_year['tech'].str.contains('E_NGAACC_N')\n",
    "orig_ngcc = all_gens_multi_year.loc[mask,:].copy()\n",
    "orig_ngcc.loc[:,'tech_desc'] = '#hydrogen combustion in a combined cycle plants for electricity generation'\n",
    "orig_ngcc.loc[:, 'tech_technologies'] = orig_ngcc.loc[:, 'tech_technologies'].str.replace('NGAA', 'H2')\n",
    "orig_ngcc.loc[:, 'tech'] = orig_ngcc.loc[:, 'tech'].str.replace('NGAA', 'H2')\n",
    "orig_ngcc.loc[:, 'Resource'] = 'H2_100'\n",
    "\n",
    "all_gens_multi_year = pd.concat([all_gens_multi_year, orig_ngcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create capacity factor dataframe\n",
    "if timeslice==1:\n",
    "    df_capfac_all= gen_variability.reset_index(drop=True)\n",
    "    df_capfac_all.columns = all_gens.loc[:,'tech'] #rename columns to match df_gen technologies\n",
    "    df_capfac_all.loc[:,'hour'] = np.tile(np.arange(0,24),365)\n",
    "    df_capfac_all.loc[((df_capfac_all['hour']>=6) & (df_capfac_all['hour']<12)), 'time_of_day_name'] = 'am'\n",
    "    df_capfac_all.loc[((df_capfac_all['hour']>=12) & (df_capfac_all['hour']<15)), 'time_of_day_name'] = 'peak'\n",
    "    df_capfac_all.loc[((df_capfac_all['hour']>=15) & (df_capfac_all['hour']<21)), 'time_of_day_name'] = 'pm'\n",
    "    df_capfac_all.loc[((df_capfac_all['hour']>=21) | (df_capfac_all['hour']<6)), 'time_of_day_name'] = 'night'\n",
    "    summer = np.arange(171*24, 265*24) #June 20 to Sep 22\n",
    "    winter = np.concatenate((np.arange(0,79*24), np.arange(355*24,8760))) #dec 21st#march 20th \n",
    "    df_capfac_all.loc[:, 'season_name'] = 'Intermediate'\n",
    "    df_capfac_all.loc[summer, 'season_name'] = 'Summer'\n",
    "    df_capfac_all.loc[winter, 'season_name'] = 'Winter'\n",
    "    df_capfac = df_capfac_all.groupby(['time_of_day_name', 'season_name']).mean().reset_index()\n",
    "    df_capfac.drop(columns=['hour'], inplace=True)\n",
    "else:\n",
    "    df_capfac= reduced_resource_profile.reset_index(drop=True)\n",
    "    df_capfac.columns = all_gens.loc[:,'tech'] #rename columns to match df_gen technologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove solar thermal from all regions except CA and SW\n",
    "#gens\n",
    "mask = (all_gens_multi_year['Resource'].str.contains('E_SOLTH')) & \\\n",
    "((~all_gens_multi_year['tech'].str.contains('-CA')) & (~all_gens_multi_year['tech'].str.contains('-SW')))\n",
    "all_gens_multi_year = all_gens_multi_year.loc[~mask,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_region(df_tech):\n",
    "    return [x[1] for x in df_tech.str.split('-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_tech(df_tech):\n",
    "    output_vals = []\n",
    "    for val in df_tech:\n",
    "        try:\n",
    "            o_val = val.split('-')[0] + '_' + val.split('-')[2]\n",
    "        except:\n",
    "            o_val = val.split('-')[0]\n",
    "        output_vals.append(o_val)\n",
    "    return output_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sql(df_table, sqlite_table, outputdB):\n",
    "#code snippets from https://github.com/EnergyModels/temoatools/tree/master/temoatools\n",
    "\n",
    "    df_table = pd.DataFrame(df_table)\n",
    "    \n",
    "    # Set-up sqlite connection\n",
    "    conn = sqlite3.connect(outputdB)\n",
    "    c = conn.cursor()\n",
    "\n",
    "    #----------\n",
    "    # sqlite file prep\n",
    "    #----------\n",
    "\n",
    "    # Create SQL command based on number of entries\n",
    "    command = 'INSERT INTO ' + sqlite_table + ' VALUES (?'\n",
    "    for i in range(len(df_table.columns)-1):\n",
    "        command = command + ',?'\n",
    "    command = command + ')'\n",
    "\n",
    "    # Execute SQL command\n",
    "    try:\n",
    "        c.executemany(command,np.array(df_table))\n",
    "    except:\n",
    "        print(command)\n",
    "        print(np.array(df_table))\n",
    "        c.executemany(command, np.array(df_table))\n",
    "\n",
    "    #----------\n",
    "    # Save(commit) the changes and close sqlite file\n",
    "    #----------\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_mult = dict()\n",
    "reg_mult['E_BECCS_N'] = new_gen.loc[new_gen['Resource']=='E_COALUSC_90CCS_N', ['region','regional_cost_multiplier']].drop_duplicates().set_index('region')\n",
    "reg_mult['H2_STO150'] = new_gen.loc[new_gen['Resource']=='E_Batt', ['region','regional_cost_multiplier']].drop_duplicates().set_index('region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regions_list = all_gens_multi_year.region.unique()\n",
    "\n",
    "# Delete old *.sqlite file (if it already exists) and copy/rename copy of temoa_schema.sqlite\n",
    "if os.path.isfile(outputdB):\n",
    "    os.remove(outputdB)\n",
    "shutil.copyfile(emptydB, outputdB)\n",
    "##remove data from tables\n",
    "conn = sqlite3.connect(outputdB)\n",
    "c = conn.cursor()\n",
    "\n",
    "table_list = c.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'Output%'\").fetchall()\n",
    "c.execute(\"UPDATE Efficiency SET tech = TRIM(tech);\") #trim spaces. Need to trim carriage return\n",
    "\n",
    "#remove entire table\n",
    "for table in [#'LifetimeProcess', #'CapacityToActivity'\n",
    "              'CapacityCredit', 'CapacityFactorTech', 'MinGenGroupWeight','MinGenGroupTarget', #'MinCapacity','StorageDuration',\n",
    "              'GrowthRateMax','GrowthRateSeed', #'MinActivity',\n",
    "               'time_periods','tech_curtailment',\n",
    "              'Output_CapacityByPeriodAndTech','Output_V_Capacity','Output_VFlow_In', 'Output_VFlow_Out',\n",
    "              'Output_Objective', 'Output_Emissions', 'Output_curtailment', 'Output_Costs',\n",
    "               'tech_groups', 'groups', 'MyopicBaseyear'\n",
    "              ]:\n",
    "    query = \"\"\"DELETE FROM \"\"\" + table\n",
    "    c.execute(query)\n",
    "    \n",
    "if timeslice==0:\n",
    "    for table in ['SegFrac', 'time_of_day', 'time_season','DemandSpecificDistribution' ]:\n",
    "        query = \"\"\"DELETE FROM \"\"\" + table\n",
    "        c.execute(query)\n",
    "        \n",
    "if single_region==0:\n",
    "    query = \"DELETE FROM EmissionLimit\"\n",
    "    c.execute(query)\n",
    "\n",
    "#delete distributed generation (from solar) from Efficiency table in original database\n",
    "#query = \"\"\"DELETE FROM Efficiency WHERE input_comm='ELCDIST_R'\"\"\"\n",
    "#c.execute(query)\n",
    "query = \"\"\"DELETE FROM Efficiency WHERE tech='IMPELC'\"\"\"\n",
    "c.execute(query)\n",
    "\n",
    "#removing upstream 'clean coal pathway, with pre-combustion retrofits'\n",
    "query = \"\"\"DELETE FROM Efficiency WHERE tech='E_CCR_COALSTM_N'\"\"\"\n",
    "c.execute(query)\n",
    "\n",
    "#deleting variable costs associated with transmission\n",
    "#query = \"\"\"DELETE FROM CostVariable WHERE tech='E_ELCTDLOSS'\"\"\"\n",
    "#c.execute(query)\n",
    "\n",
    "#modify max capacity geo techs\n",
    "tech_keep = 'E_GEOBCFS_N'\n",
    "df = pd.read_sql_query(\"SELECT * FROM MaxCapacity WHERE tech='\" + tech_keep + \"'\", conn)\n",
    "df1 = df.copy()\n",
    "df2 = df.copy()\n",
    "df1['tech'] = 'E_GEOF_N'\n",
    "df2['tech'] = 'E_GEOB_N'\n",
    "query = \"DELETE FROM MaxCapacity WHERE tech='\" + tech_keep + \"'\"\n",
    "c.execute(query)\n",
    "df_new = pd.concat([df1, df2])\n",
    "df_new.to_sql('MaxCapacity',conn, if_exists='append', index=False)\n",
    "\n",
    "\n",
    "techs_keep = ['E_BECCS_N','H2_STO150']#, 'E_H2CC_N']\n",
    "old_df_c2a = pd.read_sql(\"SELECT * FROM CapacityToActivity\", conn)\n",
    "#query = \"DELETE FROM CapacityToActivity WHERE tech NOT IN ('\" + \"','\".join(techs_keep) + \"')\"\n",
    "#c.execute(query)\n",
    "for tech_keep in techs_keep:\n",
    "    #modify the technologies that are retained from US_National for multiple regions\n",
    "    for table in table_list:\n",
    "        df_cols = c.execute(\"SELECT * FROM pragma_table_info('\" + table[0] + \"')\").fetchall()\n",
    "        df_cols = [x[1] for x in df_cols]\n",
    "        if 'tech' in df_cols:\n",
    "            df = pd.read_sql_query(\"SELECT * FROM \" + table[0] + \" WHERE tech='\" + tech_keep + \"'\", conn)\n",
    "            if table[0]=='CapacityFactorTech':\n",
    "                query = \"DELETE FROM \" + table[0] + \" WHERE tech='\" + tech_keep + \"'\"\n",
    "                c.execute(query)\n",
    "            elif (len(df)>0) & ('regions' in df.columns):\n",
    "                query = \"DELETE FROM \" + table[0] + \" WHERE tech='\" + tech_keep + \"'\"\n",
    "                c.execute(query)\n",
    "                for reg in regions_list:\n",
    "                    df_new = df.copy()\n",
    "                    df_new['regions'] = reg\n",
    "                    if table[0]=='CostInvest':\n",
    "                        df_new['cost_invest'] *= reg_mult[tech_keep].loc[reg].values[0]\n",
    "                    df_new.to_sql(table[0],conn, if_exists='append', index=False)\n",
    "\n",
    "#modify efficiency for processes that meet demands that can only be met by processes of vintage 2017\n",
    "for table in ['Efficiency','EmissionActivity']:\n",
    "    query = \"\"\"UPDATE \"\"\" + table + \"\"\" SET vintage=2020\n",
    "    WHERE vintage=2017\n",
    "    AND regions || input_comm || tech || output_comm NOT IN \n",
    "    (\n",
    "        SELECT DISTINCT\n",
    "        regions || input_comm || tech || output_comm\n",
    "        FROM \"\"\" + table + \"\"\" WHERE vintage = 2020\n",
    "    )\"\"\"\n",
    "    c.execute(query)\n",
    "\n",
    "#modify cost tables to account for these processes\n",
    "for table in ['CostFixed','CostVariable']:\n",
    "    query= \"\"\"UPDATE \"\"\" + table + \"\"\" SET vintage=2020\n",
    "        WHERE \n",
    "          vintage = 2017 \n",
    "          AND\n",
    "          regions || tech NOT IN \n",
    "        (SELECT DISTINCT\n",
    "          regions || tech\n",
    "        FROM \"\"\" + table + \"\"\" WHERE vintage = 2020)\"\"\"\n",
    "    c.execute(query)\n",
    "\n",
    "#delete all elec technologies except for those in techs_keep\n",
    "for table in ['Efficiency', 'ExistingCapacity', 'DiscountRate', 'CostVariable', 'CostFixed', 'CostInvest']:\n",
    "    query = \"DELETE FROM \" + table + \" WHERE tech IN\\\n",
    "    (SELECT tech FROM technologies WHERE sector='electric')\\\n",
    "    AND tech NOT IN ('\" + \"','\".join(techs_keep) + \"')\"\n",
    "    c.execute(query)\n",
    "\n",
    "#delete efficiencies from 2017 vintages\n",
    "for table in ['Efficiency','EmissionActivity']:\n",
    "    query = \"\"\"DELETE FROM \"\"\" + table + \"\"\" WHERE vintage=2017\"\"\"\n",
    "    c.execute(query)\n",
    "\n",
    "if single_region==1:\n",
    "    table = 'EmissionLimit'\n",
    "    query = \"\"\"DELETE FROM \"\"\" + table + \"\"\" WHERE periods=2017\"\"\"\n",
    "    c.execute(query)    \n",
    "        \n",
    "# Delete row from Efficiency if (t,v) retires at the begining of crent period (which is time_periods[i][0])\n",
    "c.execute(\"DELETE FROM Efficiency WHERE tech IN (SELECT tech FROM LifetimeProcess WHERE \\\n",
    "             LifetimeProcess.life_process+LifetimeProcess.vintage<=2020) \\\n",
    "             AND vintage IN (SELECT vintage FROM LifetimeProcess WHERE LifetimeProcess.life_process+\\\n",
    "             LifetimeProcess.vintage<=2020);\")\n",
    "\n",
    "# Delete row from Efficiency if (t,v) retires at the begining of crent period (which is time_periods[i][0])\n",
    "c.execute(\"DELETE FROM Efficiency WHERE tech IN (SELECT tech FROM LifetimeTech WHERE \\\n",
    "             LifetimeTech.life+Efficiency.vintage<=2020) AND \\\n",
    "             vintage NOT IN (SELECT vintage FROM LifetimeProcess WHERE LifetimeProcess.tech\\\n",
    "             =Efficiency.tech);\")\n",
    "\n",
    "# If row is not deleted via the last two DELETE commands, it might still be invalid for period\n",
    "#  time_periods[i][0] since they can have model default lifetime of 40 years. \n",
    "c.execute(\"DELETE FROM Efficiency WHERE tech IN (SELECT tech FROM Efficiency WHERE \\\n",
    "            40+Efficiency.vintage<=2020) AND \\\n",
    "            tech NOT IN (SELECT tech FROM LifetimeTech) AND \\\n",
    "            vintage NOT IN (SELECT vintage FROM LifetimeProcess WHERE LifetimeProcess.tech=Efficiency.tech);\")\n",
    "\n",
    "\n",
    "#remove demands, techinput split before 2020\n",
    "for table in ['Demand','TechInputSplit', 'MinActivity', 'MaxActivity','CostFixed','CostVariable']:\n",
    "    query = \"\"\"DELETE FROM \"\"\" + table +\"\"\" where periods < 2020\"\"\"\n",
    "    c.execute(query)\n",
    "for table in ['CostFixed','CostVariable']:\n",
    "    query = \"\"\"DELETE FROM \"\"\" + table +\"\"\" where vintage=2017\"\"\"\n",
    "    c.execute(query)\n",
    "for table in ['CostInvest', 'DiscountRate']:\n",
    "    query = \"\"\"DELETE FROM \"\"\" + table + \"\"\" where vintage < 2020\"\"\"\n",
    "    c.execute(query)\n",
    "    \n",
    "#if creating electricity only database, delete all other demands\n",
    "if elec_only==1:\n",
    "    query = \"DELETE FROM Efficiency WHERE output_comm IN (SELECT demand_comm FROM Demand)\"\n",
    "    c.execute(query)\n",
    "    c.execute(\"DELETE FROM Demand\")\n",
    "    c.execute(\"DELETE FROM DemandSpecificDistribution\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(outputdB, table_name, primary_key, foreign_key, key_val, references=0):\n",
    "    create_table_sql = \"\"\"CREATE TABLE IF NOT EXISTS \"\"\" + table_name + \"\"\" ( \n",
    "                         \"\"\" + primary_key + \"\"\" text primary key\"\"\"\n",
    "    \n",
    "    for key in key_val.keys():\n",
    "        create_table_sql += \"\"\", \"\"\" + key + \"\"\" \"\"\" + key_val[key] \n",
    "                         \n",
    "    if references!=0:\n",
    "        for key in references.keys():\n",
    "            create_table_sql += \"\"\", FOREIGN KEY(\"\"\" + foreign_key + \"\"\") REFERENCES \"\"\" + key + \"\"\"(\"\"\" + references[key] + \"\"\")\"\"\"\n",
    "            \n",
    "        create_table_sql += \"\"\");\"\"\"\n",
    "    else:\n",
    "        create_table_sql += \"\"\");\"\"\"\n",
    "    \n",
    "    conn = sqlite3.connect(outputdB)\n",
    "    c = conn.cursor()\n",
    "    c.execute(create_table_sql)\n",
    "    conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_mult_pkey(outputdB, table_name, primary_key, foreign_key, key_val, references=0):\n",
    "    create_table_sql = \"\"\"CREATE TABLE IF NOT EXISTS \"\"\" + table_name + \"\"\"(\"\"\"\n",
    "    \n",
    "    for key in key_val.keys():\n",
    "        create_table_sql +=  key + \"\"\" \"\"\" + key_val[key] + \"\"\", \"\"\"\n",
    "                         \n",
    "    if references!=0:\n",
    "        for key in references.keys():\n",
    "            create_table_sql += \"\"\" FOREIGN KEY(\"\"\" + foreign_key + \"\"\") REFERENCES \"\"\" + key + \"\"\"(\"\"\" + references[key] + \"\"\")\"\"\"\n",
    "            \n",
    "        #create_table_sql += \"\"\",\"\"\"\n",
    "    #else:\n",
    "        create_table_sql += \"\"\",\"\"\"\n",
    "        \n",
    "    create_table_sql+= \"\"\" PRIMARY KEY \"\"\" + primary_key  + \"\"\");\"\"\"\n",
    "    \n",
    "    conn = sqlite3.connect(outputdB)\n",
    "    c = conn.cursor()\n",
    "    c.execute(create_table_sql)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_regions(x):\n",
    "    return x.split('-')[1] + '-' + x.split('-')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def prep_tables(period, folder_name, gen_data, capfac_variability_data):\n",
    "    #PREPARE DATABASE TABLES\n",
    "    \n",
    "df_gen= all_gens_multi_year\n",
    "df_gen.rename(columns={'region':'regions'}, inplace=True)\n",
    "\n",
    "df_gen.loc[:,'regions'] = df_gen.loc[:,'regions'].str.replace('US_N','US')\n",
    "#df_gen.loc[:, 'tech'] =  df_gen.loc[:, 'Resource'] + '-' + df_gen.loc[:,'regions'] + '-' +df_gen.loc[:,'cluster'].map(int).map(str)\n",
    "\n",
    "#remove battery, pumped storage and ev_shifting (after column names have been assigned to capfac dataframe)\n",
    "df_gen = df_gen.loc[~df_gen.loc[:,'tech'].str.contains('E_LD_SHFT'),:]\n",
    "#df_gen = df_gen.loc[~df_gen.loc[:,'tech'].str.contains('battery'),:]\n",
    "#df_gen = df_gen.loc[~df_gen.loc[:,'tech'].str.contains('pumped'),:]\n",
    "#rename operating_year as vintage\n",
    "df_gen.rename(columns={'operating_year':'vintage'},inplace=True)\n",
    "df_gen= df_gen.astype({'vintage':int})\n",
    "\n",
    "#modify lifetimes of clusters post-PG runs\n",
    "df_gen.loc[:,'lifetime'] = [generator_lifetimes[y] for x in df_gen['tech'] for y in generator_lifetimes.keys() if y.lower() in x.lower()] #map lifetimes\n",
    "\n",
    "#capacityfactortech\n",
    "#remove ev_shifting\n",
    "df_capfac.drop(columns=df_capfac.columns[df_capfac.columns.str.contains('E_LD_SHFT')], inplace=True)\n",
    "#df_capfac.drop(columns=df_capfac.columns[df_capfac.columns.str.contains('battery')], inplace=True)\n",
    "#df_capfac.drop(columns=df_capfac.columns[df_capfac.columns.str.contains('pumped')], inplace=True)\n",
    "    \n",
    "intra_annual_periods = len(df_capfac)\n",
    "if timeslice==0:\n",
    "    days = intra_annual_periods/24\n",
    "    iter_val = 0\n",
    "    for day in np.arange(days)+1:\n",
    "        df_capfac.loc[iter_val:iter_val+23,'season_name'] = 'S' + str(int(day))\n",
    "        df_capfac.loc[iter_val:iter_val+23,'time_of_day_name'] = ['H' + str(int(x)) for x in np.arange(1,25)]\n",
    "        iter_val+=24\n",
    "df_capfac = df_capfac.melt(id_vars=['season_name','time_of_day_name'])\n",
    "df_capfac = df_capfac.rename(columns={'variable':'tech', 'value':'cf_tech'})\n",
    "df_capfac.loc[:,'cf_tech_notes'] = 'from Power Genome: ' + scenario\n",
    "df_capfac.insert(0,'regions',return_region(df_capfac.loc[:,'tech']))\n",
    "#df_capfac.loc[:,'regions'] = df_capfac.loc[:,'regions'].str.replace('US_N','US')\n",
    "df_capfac.loc[:,'tech'] = return_tech(df_capfac.loc[:,'tech'])\n",
    "    \n",
    "#remove techs that have constant capacity factor\n",
    "df_sum = df_capfac.groupby(by=['regions','tech']).sum().reset_index()\n",
    "df_fixed_capfac = df_sum[df_sum.cf_tech==intra_annual_periods]\n",
    "df_fixed_capfac.rename(columns={'cf_tech': 'cf_tech_max'}, inplace=True)\n",
    "df_capfac = df_capfac.merge(df_fixed_capfac, on = ['regions','tech'], how='outer')\n",
    "df_capfac = df_capfac[df_capfac.cf_tech_max!=intra_annual_periods]\n",
    "df_capfac = df_capfac.drop('cf_tech_max',axis=1)\n",
    "\n",
    "#segfrac for a chronological model has equal weights for all time periods, with a sum of 1\n",
    "df_segfrac = df_capfac.loc[:,['season_name','time_of_day_name']].copy()\n",
    "df_segfrac.drop_duplicates(inplace=True) \n",
    "df_segfrac.loc[:,'segfrac'] = 1/len(df_segfrac)\n",
    "df_segfrac.loc[:,'segfrac_notes'] = 'from Power Genome: ' + scenario\n",
    "\n",
    "#identify renewables that qualify for RPS and hydro, but not storage \n",
    "df_gen.loc[:,'renewable_nonstor'] = df_gen.loc[:,'RPS'] | df_gen.loc[:,'HYDRO'] \n",
    "df_gen.loc[df_gen['technology'].str.contains('Wind'),'renewable_nonstor'] = 1\n",
    "df_gen.loc[df_gen['technology'].str.contains('PV'),'renewable_nonstor'] = 1\n",
    "df_gen.loc[df_gen['technology'].str.contains('CSP'),'renewable_nonstor'] = 1\n",
    "df_gen.loc[df_gen['technology'].str.contains('Solar'),'renewable_nonstor'] = 1\n",
    "\n",
    "#convert heat rates of all renewables to zero\n",
    "df_gen.loc[df_gen['technology'].str.contains('Wind'),'Heat_rate_MMBTU_per_MWh'] = 0\n",
    "df_gen.loc[df_gen['technology'].str.contains('PV'),'Heat_rate_MMBTU_per_MWh'] = 0\n",
    "df_gen.loc[df_gen['technology'].str.contains('CSP'),'Heat_rate_MMBTU_per_MWh'] = 0\n",
    "df_gen.loc[df_gen['technology'].str.contains('Solar'),'Heat_rate_MMBTU_per_MWh'] = 0\n",
    "df_gen.loc[df_gen['technology'].str.contains('Geo'),'Heat_rate_MMBTU_per_MWh'] = 0\n",
    "df_gen.loc[df_gen['technology'].str.contains('Conventional Hydroelectric'),'Heat_rate_MMBTU_per_MWh'] = 0\n",
    "df_gen.loc[df_gen['technology'].str.contains('Small Hydroelectric'),'Heat_rate_MMBTU_per_MWh'] = 0\n",
    "\n",
    "#update lifetime as difference between retirement year and operating year\n",
    "mask = df_gen.loc[:,'vintage'] + df_gen.loc[:,'lifetime'] <=start_year\n",
    "df_gen.loc[mask, 'lifetime'] = start_year - df_gen.loc[mask,'vintage'] + 5\n",
    "#df_gen.loc[:,'lifetime_diff'] = df_gen.loc[:,'retirement_year'] - df_gen.loc[:,'vintage']\n",
    "#df_gen.loc[:,'lifetime_diff'].fillna(0, inplace=True)\n",
    "\n",
    "#df_gen.loc[:,'lifetime'].fillna(0, inplace=True)\n",
    "#df_gen.loc[:,'lifetime'] = df_gen.loc[:, ['lifetime', 'lifetime_diff']].max(axis=1)\n",
    "\n",
    "#capacitytoactivity for a chronological model, to normalize to annual time periods, assuming min value is hours\n",
    "df_c2a = df_gen.loc[:,['tech']].drop_duplicates()\n",
    "#remove battery and ev_shifting\n",
    "df_c2a = df_c2a.loc[~df_c2a.loc[:,'tech'].str.contains('ev_'),:]\n",
    "df_c2a = df_c2a.loc[~df_c2a.loc[:,'tech'].str.contains('battery'),:]\n",
    "df_c2a = df_c2a.loc[~df_c2a.loc[:,'tech'].str.contains('pumped'),:]\n",
    "\n",
    "df_c2a.loc[:,'c2a'] = 31.536 #8760/(days*24)\n",
    "df_c2a.loc[:,'c2a_notes'] = 'from Power Genome: ' + scenario\n",
    "df_c2a.insert(0,'regions',return_region(df_c2a.loc[:,'tech']))\n",
    "df_c2a.loc[:,'regions'] = df_c2a.loc[:,'regions'].str.replace('US_N','US')\n",
    "df_c2a.loc[:,'tech'] = return_tech(df_c2a.loc[:,'tech'])\n",
    "\n",
    "#investment, fixed and variable costs\n",
    "df_costs = df_gen.loc[:,['tech', 'capex_mw', 'Fixed_OM_cost_per_MWyr', 'Var_OM_cost_per_MWh', 'vintage']].copy()\n",
    "df_costs.loc[:,'cost_invest'] = df_costs.loc[:, 'capex_mw']/(10**3) #$/MW to #$M/GW\n",
    "df_costs.loc[:,'cost_fixed'] = df_costs.loc[:, 'Fixed_OM_cost_per_MWyr']*(10**3)/(10**6) #$/MWyr to #$M/GW-yr\n",
    "df_costs.loc[:,'cost_variable'] = df_costs.loc[:, 'Var_OM_cost_per_MWh']*(277777.78)/(10**6) #$/MWh to #$M/PJ\n",
    "df_costs.loc[:,'cost_invest_units'] = '$M/GW'\n",
    "df_costs.loc[:,'cost_invest_notes'] = 'from Power Genome: ' + scenario\n",
    "df_costs.loc[:,'cost_fixed_units'] = '$M/GWyr'\n",
    "df_costs.loc[:,'cost_fixed_notes'] = 'from Power Genome: ' + scenario\n",
    "df_costs.loc[:,'cost_variable_units'] = '$M/PJ'\n",
    "df_costs.loc[:,'cost_variable_notes'] = 'from Power Genome: ' + scenario\n",
    "df_costs.insert(0,'regions',return_region(df_costs.loc[:,'tech']))\n",
    "df_costs.loc[:,'tech'] = return_tech(df_costs.loc[:,'tech'])\n",
    "\n",
    "df_cost_invest = df_costs.loc[:,['regions','tech','vintage','cost_invest','cost_invest_units','cost_invest_notes']].copy()\n",
    "df_cost_invest = df_cost_invest.loc[df_cost_invest.cost_invest>0]\n",
    "\n",
    "df_costv = df_costs.loc[:,['regions','tech','vintage','cost_variable','cost_variable_units','cost_variable_notes']].drop_duplicates()\n",
    "df_cost_variable = pd.DataFrame(np.repeat(df_costv.values, len(all_periods), axis=0), columns= df_costv.columns)\n",
    "df_cost_variable.insert(1,'periods',int((len(df_cost_variable)/len(all_periods)))*all_periods)\n",
    "df_cost_variable = df_cost_variable[df_cost_variable.vintage<=df_cost_variable.periods]\n",
    "\n",
    "df_costf = df_costs.loc[:,['regions','tech','vintage','cost_fixed','cost_fixed_units','cost_fixed_notes']]\n",
    "df_cost_fixed = pd.DataFrame(np.repeat(df_costf.values, len(all_periods), axis=0), columns= df_costf.columns)\n",
    "df_cost_fixed.insert(1,'periods',int((len(df_cost_fixed)/len(all_periods)))*all_periods)\n",
    "df_cost_fixed = df_cost_fixed[df_cost_fixed.vintage<=df_cost_fixed.periods]\n",
    "\n",
    "\n",
    "#efficiency\n",
    "df_efficiency = df_gen.loc[:,['Resource','tech', 'Heat_rate_MMBTU_per_MWh','renewable_nonstor', 'vintage', 'flag', 'sector', 'tech_desc', 'tech_category']].copy()\n",
    "df_efficiency.rename(columns={'Resource':'input_comm'}, inplace=True)\n",
    "#convert technologies with no heat rate to 100% efficiency, by setting the heat rate to 3412.0/1000 MMBTU/MWh\n",
    "df_efficiency.loc[(df_efficiency.loc[:,'Heat_rate_MMBTU_per_MWh']==0),'Heat_rate_MMBTU_per_MWh'] = 3412.0/1000\n",
    "df_efficiency.loc[:, 'output_comm'] = df_efficiency.loc[:,'renewable_nonstor'].apply(lambda x: 'ELCP_Renewables' if x ==1 else 'ELCP')\n",
    "mask = df_efficiency.loc[:,'tech'].str.contains('Batt')\n",
    "df_efficiency.loc[mask, 'output_comm'] = 'ELC'\n",
    "df_efficiency.loc[:,'efficiency'] = 3412.0/(df_efficiency.loc[:,'Heat_rate_MMBTU_per_MWh']*1000)\n",
    "df_efficiency.loc[:,'efficiency'].fillna(1.0, inplace=True)\n",
    "df_efficiency.drop(columns=['Heat_rate_MMBTU_per_MWh','renewable_nonstor'], inplace=True)\n",
    "df_efficiency.loc[:,'eff_notes'] = 'from Power Genome: ' + scenario\n",
    "#remove battery and ev_shifting\n",
    "df_efficiency = df_efficiency.loc[~df_efficiency.loc[:,'tech'].str.contains('ev_'),:]\n",
    "df_efficiency = df_efficiency.loc[~df_efficiency.loc[:,'tech'].str.contains('battery'),:]\n",
    "df_efficiency.insert(0,'regions',return_region(df_efficiency.loc[:,'tech']))\n",
    "df_efficiency.loc[:,'tech'] = return_tech(df_efficiency.loc[:,'tech'])\n",
    "\n",
    "#ramp up and down fractions\n",
    "df_ramp = df_gen.loc[(df_gen.loc[:,'Ramp_Up_percentage']>0) | (df_gen.loc[:,'Ramp_Dn_percentage']>0), ['regions','tech', 'Ramp_Up_percentage', 'Ramp_Dn_percentage']].copy()\n",
    "df_ramp.rename(columns={'Ramp_Up_percentage': 'ramp_up', 'Ramp_Dn_percentage': 'ramp_down'}, inplace=True)\n",
    "df_ramp.loc[:,'tech'] = return_tech(df_ramp.loc[:,'tech'])\n",
    "df_ramp = df_ramp[(df_ramp.ramp_up<1) | (df_ramp.ramp_down<1)]\n",
    "\n",
    "#lifetime\n",
    "#df_lifetime = df_gen.loc[:,['tech','lifetime']].copy()\n",
    "#df_lifetime.insert(0,'regions',return_region(df_lifetime.loc[:,'tech']))\n",
    "#df_lifetime.loc[:,'tech'] = return_tech(df_lifetime.loc[:,'tech'])\n",
    "\n",
    "#existing capacity\n",
    "df_ex_cap = df_gen.loc[:, ['tech','Existing_Cap_MW', 'vintage' ]]\n",
    "df_ex_cap = df_ex_cap.loc[df_ex_cap.loc[:,'Existing_Cap_MW']>0,:]\n",
    "df_ex_cap.loc[:,'exist_cap'] = df_ex_cap.loc[:,'Existing_Cap_MW']/1000 #GW\n",
    "df_ex_cap.drop(columns=['Existing_Cap_MW'], inplace=True)\n",
    "df_ex_cap.loc[:,'exist_cap_units'] = 'GW'\n",
    "df_ex_cap.loc[:,'exist_cap_notes'] = 'from Power Genome: ' + scenario\n",
    "##FIX THIS for zero value years for EV load shifting, removing this row for now\n",
    "df_ex_cap = df_ex_cap.loc[df_ex_cap.loc[:,'vintage']!=0,:]\n",
    "df_ex_cap.insert(0,'regions',return_region(df_ex_cap.loc[:,'tech']))\n",
    "df_ex_cap.loc[:,'tech'] = return_tech(df_ex_cap.loc[:,'tech'])\n",
    "\n",
    "#discount rate\n",
    "df_wacc = df_gen.loc[(df_gen.loc[:,'wacc_nominal']>0), ['tech', 'vintage', 'wacc_nominal']].copy()\n",
    "df_wacc.rename(columns={'wacc_nominal': 'tech_rate'}, inplace=True)\n",
    "df_wacc.loc[:,'tech_rate_notes'] = 'from Power Genome: ' + scenario\n",
    "df_wacc.insert(0,'regions',return_region(df_wacc.loc[:,'tech']))\n",
    "df_wacc.loc[:,'tech'] = return_tech(df_wacc.loc[:,'tech'])\n",
    "\n",
    "#capital recovery years\n",
    "df_cap_rec_years = df_gen.loc[(df_gen.loc[:,'cap_recovery_years']>0),['tech', 'cap_recovery_years']].copy()\n",
    "df_cap_rec_years.rename(columns={'cap_recovery_years': 'loan'}, inplace=True)\n",
    "df_cap_rec_years.loc[:,'loan_notes'] = 'from Power Genome: ' + scenario\n",
    "df_cap_rec_years.insert(0,'regions',return_region(df_cap_rec_years.loc[:,'tech']))\n",
    "df_cap_rec_years.loc[:,'tech'] = return_tech(df_cap_rec_years.loc[:,'tech'])\n",
    "\n",
    "#lifetime\n",
    "df_lifetime = df_gen.loc[(df_gen.loc[:,'lifetime']>0),['tech', 'lifetime']].copy()\n",
    "df_lifetime.rename(columns={'lifetime': 'life'}, inplace=True)\n",
    "df_lifetime.loc[:,'life_notes'] = 'from Power Genome: ' + scenario\n",
    "df_lifetime.insert(0,'regions',return_region(df_lifetime.loc[:,'tech']))\n",
    "df_lifetime.loc[:,'tech'] = return_tech(df_lifetime.loc[:,'tech'])\n",
    "\n",
    "#modify fixed and variable costs based on lifetime\n",
    "df_cost_variable_merge = df_cost_variable.merge(df_lifetime, on=['regions','tech'])\n",
    "df_cost_variable_merge['retirement_year'] = df_cost_variable_merge['vintage'] + df_cost_variable_merge['life']\n",
    "df_cost_variable = df_cost_variable_merge[df_cost_variable_merge['retirement_year']>df_cost_variable_merge['periods']]\n",
    "df_cost_variable = df_cost_variable.drop(['life','life_notes','retirement_year'], axis=1)\n",
    "\n",
    "df_cost_fixed_merge = df_cost_fixed.merge(df_lifetime, on=['regions','tech'])\n",
    "df_cost_fixed_merge['retirement_year'] = df_cost_fixed_merge['vintage'] + df_cost_fixed_merge['life']\n",
    "df_cost_fixed = df_cost_fixed_merge[df_cost_fixed_merge['retirement_year']>df_cost_fixed_merge['periods']]\n",
    "df_cost_fixed = df_cost_fixed.drop(['life','life_notes','retirement_year'], axis=1)\n",
    "\n",
    "#capacity credit\n",
    "#modify capacity credit for renewables\n",
    "df_gen.loc[df_gen.tech.str.contains('WND'),'CapRes'] = 0.15\n",
    "df_gen.loc[df_gen.tech.str.contains('SOLPVCEN'),'CapRes'] = 0.15\n",
    "df_gen.loc[df_gen.tech.str.contains('SOLTHCEN_N'),'CapRes'] = 0.15\n",
    "df_gen.loc[df_gen.tech.str.contains('SOLPV_R'),'CapRes'] = 0.15\n",
    "\n",
    "df_ccredit = df_gen.loc[df_gen.loc[:,'CapRes']>0, ['tech', 'vintage', 'CapRes']].copy()\n",
    "df_ccredit  = pd.DataFrame(np.repeat(df_ccredit.values, len(all_periods), axis=0), columns= df_ccredit.columns)\n",
    "df_ccredit.insert(0,'periods',int((len(df_ccredit)/len(all_periods)))*all_periods)\n",
    "df_ccredit.rename(columns={'CapRes': 'cf_tech'}, inplace=True)\n",
    "df_ccredit.loc[:,'cf_tech_notes'] = 'from Power Genome: ' + scenario\n",
    "df_ccredit.insert(0,'regions',return_region(df_ccredit.loc[:,'tech']))\n",
    "df_ccredit.loc[:,'tech'] = return_tech(df_ccredit.loc[:,'tech'])\n",
    "mask = df_ccredit.loc[:,'periods']>=df_ccredit.loc[:,'vintage']\n",
    "df_ccredit = df_ccredit.loc[mask,:]\n",
    "\n",
    "#max capacity, for renewables, primarily\n",
    "df_maxcap = df_gen.loc[(df_gen.loc[:,'Max_Cap_MW']>0),['tech','Max_Cap_MW','vintage']].copy()\n",
    "df_maxcap.rename(columns={'Max_Cap_MW':'maxcap', 'vintage':'periods'}, inplace=True)\n",
    "df_maxcap.loc[:,'maxcap'] = df_maxcap.loc[:,'maxcap']/1000 #convert MW to GW\n",
    "df_maxcap.loc[:,'maxcap_units'] = 'GW' #convert MW to GW\n",
    "df_maxcap.loc[:,'maxcap_tech_notes'] = 'from Power Genome: ' + scenario\n",
    "df_maxcap.insert(0,'regions',return_region(df_maxcap.loc[:,'tech']))\n",
    "df_maxcap.loc[:,'tech'] = return_tech(df_maxcap.loc[:,'tech'])\n",
    "\n",
    "#load, estimating demand specific distribution, same distribution for all demands\n",
    "if timeslice==0:\n",
    "    df_load_i = reduced_load_profile\n",
    "    ind = list(reduced_load_profile.columns).index('Time_index')\n",
    "    df_load_i = reduced_load_profile[reduced_load_profile.columns[ind+1:]]\n",
    "    iter_val = 0\n",
    "    for day in np.arange(days)+1:\n",
    "        df_load_i.loc[iter_val:iter_val+23,'season_name'] = 'S' + str(int(day))\n",
    "        df_load_i.loc[iter_val:iter_val+23,'time_of_day_name'] = ['H' + str(int(x)) for x in np.arange(1,25)]\n",
    "        iter_val+=24\n",
    "    df_load_i = df_load_i.melt(id_vars=['season_name','time_of_day_name'])\n",
    "    df_load_i = df_load_i.rename(columns={'variable':'regions', 'value':'dds_vals'})\n",
    "    df_load_i.loc[:, 'regions'] = df_load_i.loc[:, 'regions'].str.replace('US_N','US')\n",
    "\n",
    "    df_load = pd.DataFrame()\n",
    "    if elec_only!=1:\n",
    "        demns = ['RSC', 'RSH','RLT','ROELC','RWH','CLT','COELC','COEELC','CSC','CSH','CWH']\n",
    "    else:\n",
    "        demns = ['ELC_dem']\n",
    "    for demn in demns:\n",
    "        for region in df_load_i.regions.unique():\n",
    "            df_load_i.loc[(df_load_i.regions==region),'demand_name'] = demn\n",
    "            df_load_i.loc[(df_load_i.regions==region),'dds'] = df_load_i.loc[(df_load_i.regions==region),'dds_vals']/df_load_i.loc[(df_load_i.regions==region),'dds_vals'].sum()\n",
    "            df_load = pd.concat([df_load, df_load_i.loc[(df_load_i.regions==region),['regions','season_name','time_of_day_name','demand_name','dds']]])\n",
    "    df_load.loc[:,'dds_notes'] = 'from Power Genome: ' + scenario  \n",
    "    df_load.loc[:,'regions'] = df_load.loc[:,'regions'].str.replace('z1','US')\n",
    "\n",
    "if len(settings['region_aggregations'])>1:\n",
    "\n",
    "    #transmission efficiency\n",
    "    df_trans_efficiency = pd.DataFrame(columns = df_efficiency.columns)\n",
    "    df_trans_efficiency.loc[:,'regions']  = transmission.loc[:,'Transmission Path Name'].str.replace('_to_','-')\n",
    "    df_trans_efficiency.loc[:,'input_comm'] = 'ELC'\n",
    "    df_trans_efficiency.loc[:,'tech'] = 'E_TRANS_R'\n",
    "    df_trans_efficiency.loc[:,'vintage'] = all_periods[0]-1\n",
    "    df_trans_efficiency.loc[:,'output_comm'] = 'ELC'\n",
    "    df_trans_efficiency.loc[:,'efficiency'] = 1 - transmission.loc[:,'Line_Loss_Percentage']\n",
    "    df_trans_efficiency.loc[:,'eff_notes'] = 'from Power Genome: ' + scenario\n",
    "    df_trans_efficiency.loc[:,'flag'] = 'p'\n",
    "    df_trans_efficiency.loc[:,'sector'] = 'electric'\n",
    "    df_trans_efficiency.loc[:,'tech_desc'] = '#electricity transmission'\n",
    "    \n",
    "    df_efficiency = pd.concat([df_efficiency, df_trans_efficiency])\n",
    "    df_trans_efficiency['regions'] = df_trans_efficiency['regions'].apply(swap_regions)\n",
    "    df_efficiency = pd.concat([df_efficiency, df_trans_efficiency])\n",
    "    df_trans_efficiency_n= df_trans_efficiency.copy()\n",
    "    \n",
    "    #transmission_efficiency_new\n",
    "    df_trans_efficiency = pd.DataFrame(columns = df_efficiency.columns)\n",
    "    df_trans_efficiency.loc[:,'regions']  = transmission.loc[:,'Transmission Path Name'].str.replace('_to_','-')\n",
    "    df_trans_efficiency.loc[:,'vintage'] = all_periods[0]\n",
    "    df_trans_efficiency.loc[:,'input_comm'] = 'ELC'\n",
    "    df_trans_efficiency.loc[:,'tech'] = 'E_TRANS_N'\n",
    "    df_trans_efficiency.loc[:,'output_comm'] = 'ELC'\n",
    "    df_trans_efficiency.loc[:,'efficiency'] = 1 - transmission.loc[:,'Line_Loss_Percentage']\n",
    "    df_trans_efficiency.loc[:,'eff_notes'] = 'from Power Genome: ' + scenario\n",
    "    df_trans_efficiency.loc[:,'flag'] = 'p'\n",
    "    df_trans_efficiency.loc[:,'sector'] = 'electric'\n",
    "    df_trans_efficiency.loc[:,'tech_desc'] = '#new electricity transmission'\n",
    "\n",
    "    df_trans_efficiency_r = pd.DataFrame(np.repeat(df_trans_efficiency.values, len(all_periods), axis=0), columns= df_trans_efficiency.columns)\n",
    "    df_trans_efficiency_r.loc[:,'vintage'] = np.tile(all_periods, len(df_trans_efficiency))\n",
    "    \n",
    "    df_efficiency = pd.concat([df_efficiency, df_trans_efficiency_r])\n",
    "    df_trans_efficiency_r['regions'] = df_trans_efficiency_r['regions'].apply(swap_regions)\n",
    "    df_efficiency = pd.concat([df_efficiency, df_trans_efficiency_r])\n",
    "\n",
    "    #transmission investment costs \n",
    "    df_trans_c = pd.DataFrame(columns = df_cost_invest.columns)\n",
    "    df_trans_c.loc[:,'regions']  = transmission.loc[:,'Transmission Path Name'].str.replace('_to_','-')\n",
    "    df_trans_c.loc[:,'vintage'] = all_periods[0]\n",
    "    df_trans_c.loc[:,'tech'] = 'E_TRANS_N'\n",
    "    #df_trans_c.loc[:,'vintage'] = all_periods[0]-1\n",
    "    df_trans_c.loc[:, 'cost_invest'] = transmission.loc[:, 'Line_Reinforcement_Cost_per_MW_yr']*(10**3)/(10**6) #$/MW to #$M/GW\n",
    "    df_trans_c.loc[:, 'cost_invest_units'] = '$M/GW'\n",
    "    df_trans_c.loc[:, 'cost_invest_notes'] = 'from Power Genome: ' + scenario\n",
    "    df_trans_costs = pd.DataFrame(np.repeat(df_trans_c.values, len(all_periods), axis=0), columns= df_trans_c.columns)\n",
    "    df_trans_costs.loc[:,'vintage'] = np.tile(all_periods, len(df_trans_c))\n",
    "    \n",
    "    df_cost_invest = pd.concat([df_cost_invest, df_trans_costs])\n",
    "    df_trans_costs['regions'] = df_trans_costs['regions'].apply(swap_regions)\n",
    "    df_cost_invest = pd.concat([df_cost_invest, df_trans_costs])\n",
    "    \n",
    "    #transmission investment wacc\n",
    "    df_trans_wacc = df_trans_costs[['regions', 'tech', 'vintage']].copy().drop_duplicates()\n",
    "    df_trans_wacc.loc[:,'tech_rate'] = 0.069 #from PG example system settings file\n",
    "    df_trans_wacc.loc[:, 'tech_rate_notes'] = 'from Power Genome: ' + scenario\n",
    "\n",
    "    df_wacc = pd.concat([df_wacc, df_trans_wacc])\n",
    "    df_trans_wacc['regions'] = df_trans_wacc['regions'].apply(swap_regions)\n",
    "    df_wacc = pd.concat([df_wacc, df_trans_wacc])\n",
    "    \n",
    "    #new transmission lifetime\n",
    "    df_trans_lifetime = df_trans_c[['regions', 'tech']].copy().drop_duplicates()\n",
    "    df_trans_lifetime.loc[:,'life'] = 60 #from PG example system settings file\n",
    "    df_trans_lifetime.loc[:, 'life_notes'] = 'from Power Genome: ' + scenario\n",
    "\n",
    "    df_lifetime = pd.concat([df_lifetime, df_trans_lifetime])\n",
    "    df_trans_lifetime['regions'] = df_trans_lifetime['regions'].apply(swap_regions)\n",
    "    df_lifetime = pd.concat([df_lifetime, df_trans_lifetime])\n",
    "\n",
    "    #trans existing capacity\n",
    "    df_trans_cap = pd.DataFrame()\n",
    "\n",
    "    df_trans_cap.loc[:,'regions']  = transmission.loc[:,'Transmission Path Name'].str.replace('_to_','-')\n",
    "    df_trans_cap.loc[:,'vintage']  = all_periods[0]-1\n",
    "\n",
    "    df_trans_cap.loc[:,'exist_cap'] = abs(transmission.loc[:,['Line_Max_Flow_MW', 'Line_Min_Flow_MW']]).max(axis=1).values/1000\n",
    "\n",
    "    df_trans_cap.insert(1,'tech', 'E_TRANS_R')\n",
    "    df_trans_cap.loc[:,'exist_cap_units'] = 'GW'\n",
    "    df_trans_cap.loc[:,'exist_cap_notes'] = 'from Power Genome: ' + scenario\n",
    "    \n",
    "    df_ex_cap = pd.concat([df_ex_cap, df_trans_cap])\n",
    "    df_trans_cap['regions'] = df_trans_cap['regions'].apply(swap_regions)\n",
    "    df_ex_cap = pd.concat([df_ex_cap, df_trans_cap])\n",
    "    \n",
    "    #capacity2activity\n",
    "    df_trans_gen = pd.concat([df_trans_efficiency_r, df_trans_efficiency_n])\n",
    "    df_c2a_trans = df_trans_gen[['regions','tech']].drop_duplicates()\n",
    "\n",
    "    df_c2a_trans.loc[:,'c2a'] = 31.536 #8760/(days*24)\n",
    "    df_c2a_trans.loc[:,'c2a_notes'] = 'from Power Genome: ' + scenario\n",
    "    df_c2a = pd.concat([df_c2a, df_c2a_trans])\n",
    "\n",
    "#size of individual units in new technologies cluster\n",
    "df_technology_new_cluster = df_gen.loc[df_gen['Existing_Cap_MW'].isna(),['tech', 'Cap_size']]\n",
    "df_technology_new_cluster.insert(0,'regions' ,return_region(df_technology_new_cluster.loc[:,'tech']))\n",
    "df_technology_new_cluster['tech'] =  return_tech(df_technology_new_cluster.loc[:,'tech'])\n",
    "df_technology_new_cluster.rename(columns={'Cap_size':'cap_size'}, inplace=True)\n",
    "df_technology_new_cluster['cap_size'] /= 1000 #convert MW to GW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add input_comm from commodities list\n",
    "tech_wo_clusters = df_efficiency['tech'].map(lambda x: '_'.join(x.split('_')[0:-1]) if x.split('_')[-1].isnumeric() else x)\n",
    "df_efficiency.loc[:,'input_comm'] = tech_wo_clusters.map({k: v[0] for k,v in input_table.items()})\n",
    "\n",
    "#duplicate bio to have multiple inputs\n",
    "bio = df_efficiency[df_efficiency.loc[:,'tech']=='E_BIO_N']\n",
    "bio.loc[:, 'input_comm'] = input_table['E_BIO_N'][1]\n",
    "df_efficiency = pd.concat([df_efficiency, bio])\n",
    "bio.loc[:, 'input_comm'] = input_table['E_BIO_N'][2]\n",
    "df_efficiency = pd.concat([df_efficiency, bio])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add nox and SO2 emissions to other electric power plants\n",
    "#technologies affected:\n",
    "conn = sqlite3.connect(emptydB)\n",
    "c = conn.cursor()\n",
    "techs_so2 = ['E_NGASTM_R', 'E_NGACT_R']\n",
    "df_emiss = pd.read_sql(\"SELECT * FROM EmissionActivity WHERE emis_comm='so2_ELC' AND tech in ('\" + \"','\".join(list(techs_so2)) +\\\n",
    "                       \"' )\", conn)\n",
    "#query = \"DELETE FROM EmissionActivity WHERE emis_comm='co2' AND tech in \\\n",
    "#('\" + \"','\".join(list(techs_so2)) + \"' )\"\n",
    "emis_group = df_emiss.groupby('tech').min()\n",
    "\n",
    "df_new_emiss = pd.DataFrame()\n",
    "for tech in techs_so2:\n",
    "    emis_techs = df_efficiency.loc[df_efficiency['tech'].str.contains(tech), :]\n",
    "    emis_techs.loc[:,'emis_act'] = emis_group.loc[emis_group.index.str.contains(tech), 'emis_act'].values[0]\n",
    "    emis_techs['emis_comm'] = 'so2_ELC'\n",
    "    emis_techs['emis_act_units'] = 'kt/PJout'\n",
    "    emis_techs['emis_act_notes'] = 'from original national db'\n",
    "    df_new_emiss = pd.concat([df_new_emiss,emis_techs ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techs_nox = ['E_NGASTM_R', 'E_NGACT_R', 'E_NGACC_R', 'E_NGACC_CCS_N']\n",
    "\n",
    "df_emiss = pd.read_sql(\"SELECT * FROM EmissionActivity WHERE emis_comm='nox_ELC' AND tech in ('\" + \"','\".join(list(techs_nox)) +\\\n",
    "                       \"' )\", conn)\n",
    "emis_group = df_emiss.groupby('tech').min()\n",
    "for tech in techs_nox:\n",
    "    emis_techs = df_efficiency.loc[df_efficiency['tech'].str.contains(tech), :]\n",
    "    emis_techs.loc[:,'emis_act'] = emis_group.loc[emis_group.index.str.contains(tech), 'emis_act'].values[0]\n",
    "    emis_techs['emis_comm'] = 'nox_ELC'\n",
    "    emis_techs['emis_act_units'] = 'kt/PJout'\n",
    "    emis_techs['emis_act_notes'] = 'from original national db'\n",
    "    if tech=='E_NGACC_CCS_N':\n",
    "        emis_techs['tech'] = 'E_NGACC_CCS_ZERO_N'\n",
    "    df_new_emiss = pd.concat([df_new_emiss,emis_techs ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add CO2 output commodity to the efficiency tables\n",
    "# co2 emissions factors from here: https://www.eia.gov/tools/faqs/faq.php?id=73&t=11\n",
    "emiss_dict = {}\n",
    "emiss_dict['COALSTM_N'] = 88.42 #using the min of what's in the db at this time 205.7/2.2/(9.478e+5)*1000*1000 # lbs/MMBTU to kt/PJ\n",
    "emiss_dict['E_NGA'] = 50 # using the min of what's in the db 117.0/2.2/(9.478e+5)*1000*1000 # lbs/MMBTU to kt/PJ\n",
    "emiss_dict['COALIGCC_N'] = 88.42 #using the min of what's in the db at this time 205.7/2.2/(9.478e+5)*1000*1000 # lbs/MMBTU to kt/PJ\n",
    "\n",
    "mask = df_efficiency['tech'].str.contains('CCS')\n",
    "ccs_techs = df_efficiency.loc[mask, :].copy()\n",
    "ccs_techs.loc[:,'fuel_emis'] = ccs_techs['input_comm'].map(emiss_dict)\n",
    "ccs_techs.loc[:,'output_comm'] = 'co2_CCS'\n",
    "ccs_techs.loc[:,'co2_removal_eff'] = 0.9\n",
    "ccs_techs.loc[(ccs_techs.loc[:,'tech'].str.contains('ZERO')),'co2_removal_eff'] = 1\n",
    "\n",
    "ccs_techs.loc[:,'to_split'] = 1 - ccs_techs.loc[:,'efficiency']/( ccs_techs.loc[:,'efficiency']+ (ccs_techs.loc[:,'co2_removal_eff']*ccs_techs.loc[:,'fuel_emis']/ccs_techs.loc[:,'efficiency']))\n",
    "\n",
    "#modify the outputs of all ccs techs to be PJ + co2 emissions\n",
    "df_efficiency.loc[mask,'efficiency'] = ccs_techs.loc[:,'efficiency']+  ccs_techs.loc[:,'co2_removal_eff']*ccs_techs.loc[:,'fuel_emis']/ccs_techs.loc[:,'efficiency']\n",
    "ccs_techs.loc[:,'efficiency'] = ccs_techs.loc[:,'co2_removal_eff']*ccs_techs.loc[:,'fuel_emis']/ccs_techs.loc[:,'efficiency'] #+ ccs_techs.loc[:,'efficiency']\n",
    "\n",
    "df_efficiency = pd.concat([df_efficiency, ccs_techs[df_efficiency.columns]])\n",
    "\n",
    "# add CO2 output to the emissionsactivity tables\n",
    "conn = sqlite3.connect(outputdB)\n",
    "c = conn.cursor()\n",
    "df_emiss = pd.read_sql(\"SELECT * FROM EmissionActivity WHERE emis_comm='co2' AND tech in ('\" + \"','\".join(list(ccs_techs.tech.unique())) +\\\n",
    "                       \"' )\", conn)\n",
    "query = \"DELETE FROM EmissionActivity WHERE emis_comm='co2' AND tech in \\\n",
    "('\" + \"','\".join(list(ccs_techs.tech.unique())) + \"' )\"\n",
    "c.execute(query)\n",
    "\n",
    "ccs_techs.insert(1,'emis_comm','co2')\n",
    "ccs_techs['emis_act'] = -ccs_techs['efficiency']\n",
    "ccs_techs['emis_act_units'] = 'kt/PJout'\n",
    "ccs_techs['emis_act_notes'] = 'estimated using power plant efficiency'\n",
    "ccs_techs['to_split_notes'] = 'estimated using power plant efficiency'\n",
    "ccs_techs['periods'] = ccs_techs['vintage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for CCS techs, add elec vs CO2 ratios to techoutput split table, and modify the efficiency table\n",
    "df_tosplit = pd.read_sql(\"SELECT * FROM TechOutputSplit\", conn)\n",
    "\n",
    "df_tosplit_ccs = ccs_techs[df_tosplit.columns].copy()\n",
    "df_tosplit_ccs_elec = df_tosplit_ccs.copy()\n",
    "df_tosplit_ccs_elec.loc[:,'output_comm'] = 'ELCP'\n",
    "df_tosplit_ccs_elec.loc[:,'to_split'] = 1 - df_tosplit_ccs_elec.loc[:,'to_split']\n",
    "df_tosplit_ccs = pd.concat([df_tosplit_ccs, df_tosplit_ccs_elec])\n",
    "\n",
    "#update ccs_techs output commodity to be electricity\n",
    "ccs_techs.loc[:,'output_comm'] = 'ELCP'\n",
    "\n",
    "#remove modified CCS techs from existing techoutputsplit table\n",
    "query = \"DELETE FROM TechOutputSplit WHERE tech in \\\n",
    "('\" + \"','\".join(list(ccs_techs.tech.unique())) + \"' )\"\n",
    "c.execute(query)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change existing wind and solar efficiencies to 1\n",
    "df_efficiency.loc[(df_efficiency.loc[:,'tech'].str.contains('wind')) & (df_efficiency.loc[:,'vintage']<start_year), 'efficiency'] = 1\n",
    "df_efficiency.loc[(df_efficiency.loc[:,'tech'].str.contains('solar')) & (df_efficiency.loc[:,'vintage']<start_year), 'efficiency'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change output_comm for residential PV to ELCDIST_R\n",
    "df_efficiency.loc[df_efficiency.loc[:,'tech']=='E_SOLPVENDUSE_N', 'output_comm'] = 'ELCDIST_R'\n",
    "\n",
    "#convert residential solar PV into three clusters\n",
    "#capacity factors, investment cost, FOM, VOM, efficiency, capacitytoactivity, discount rate, lifetime tech,  \n",
    "#technologies, tech_curtail, tech_new_cluster\n",
    "    \n",
    "respv = df_efficiency.loc[df_efficiency.loc[:,'tech']=='E_SOLPVENDUSE_N',:]\n",
    "respv.loc[:,'tech'] = 'E_SOLPVENDUSE_N_2'\n",
    "df_efficiency.loc[df_efficiency.loc[:,'tech']=='E_SOLPVENDUSE_N','tech'] = 'E_SOLPVENDUSE_N_1'\n",
    "df_efficiency = pd.concat([df_efficiency, respv])\n",
    "respv.loc[:,'tech'] = 'E_SOLPVENDUSE_N_3'\n",
    "df_efficiency = pd.concat([df_efficiency, respv])\n",
    "\n",
    "respv = df_cost_invest.loc[df_cost_invest.loc[:,'tech']=='E_SOLPVENDUSE_N',:]\n",
    "respv.loc[:,'tech'] = 'E_SOLPVENDUSE_N_2'\n",
    "df_cost_invest.loc[df_cost_invest.loc[:,'tech']=='E_SOLPVENDUSE_N','tech'] = 'E_SOLPVENDUSE_N_1'\n",
    "df_cost_invest = pd.concat([df_cost_invest, respv])\n",
    "respv.loc[:,'tech'] = 'E_SOLPVENDUSE_N_3'\n",
    "df_cost_invest = pd.concat([df_cost_invest, respv])\n",
    "\n",
    "respv = df_cost_variable.loc[df_cost_variable.loc[:,'tech']=='E_SOLPVENDUSE_N',:]\n",
    "respv.loc[:,'tech'] = 'E_SOLPVENDUSE_N_2'\n",
    "df_cost_variable.loc[df_cost_variable.loc[:,'tech']=='E_SOLPVENDUSE_N','tech'] = 'E_SOLPVENDUSE_N_1'\n",
    "df_cost_variable = pd.concat([df_cost_variable, respv])\n",
    "respv.loc[:,'tech'] = 'E_SOLPVENDUSE_N_3'\n",
    "df_cost_variable = pd.concat([df_cost_variable, respv])\n",
    "\n",
    "respv = df_cost_fixed.loc[df_cost_fixed.loc[:,'tech']=='E_SOLPVENDUSE_N',:]\n",
    "respv.loc[:,'tech'] = 'E_SOLPVENDUSE_N_2'\n",
    "df_cost_fixed.loc[df_cost_fixed.loc[:,'tech']=='E_SOLPVENDUSE_N','tech'] = 'E_SOLPVENDUSE_N_1'\n",
    "df_cost_fixed = pd.concat([df_cost_fixed, respv])\n",
    "respv.loc[:,'tech'] = 'E_SOLPVENDUSE_N_3'\n",
    "df_cost_fixed = pd.concat([df_cost_fixed, respv])\n",
    "\n",
    "respv = df_c2a.loc[df_c2a.loc[:,'tech']=='E_SOLPVENDUSE_N',:]\n",
    "respv.loc[:,'tech'] = 'E_SOLPVENDUSE_N_2'\n",
    "df_c2a.loc[df_c2a.loc[:,'tech']=='E_SOLPVENDUSE_N','tech'] = 'E_SOLPVENDUSE_N_1'\n",
    "df_c2a = pd.concat([df_c2a, respv])\n",
    "respv.loc[:,'tech'] = 'E_SOLPVENDUSE_N_3'\n",
    "df_c2a = pd.concat([df_c2a, respv])\n",
    "\n",
    "respv = df_wacc.loc[df_wacc.loc[:,'tech']=='E_SOLPVENDUSE_N',:]\n",
    "respv.loc[:,'tech'] = 'E_SOLPVENDUSE_N_2'\n",
    "df_wacc.loc[df_wacc.loc[:,'tech']=='E_SOLPVENDUSE_N','tech'] = 'E_SOLPVENDUSE_N_1'\n",
    "df_wacc = pd.concat([df_wacc, respv])\n",
    "respv.loc[:,'tech'] = 'E_SOLPVENDUSE_N_3'\n",
    "df_wacc = pd.concat([df_wacc, respv])\n",
    "\n",
    "respv = df_lifetime.loc[df_lifetime.loc[:,'tech']=='E_SOLPVENDUSE_N',:]\n",
    "respv.loc[:,'tech'] = 'E_SOLPVENDUSE_N_2'\n",
    "df_lifetime.loc[df_lifetime.loc[:,'tech']=='E_SOLPVENDUSE_N','tech'] = 'E_SOLPVENDUSE_N_1'\n",
    "df_lifetime = pd.concat([df_lifetime, respv])\n",
    "respv.loc[:,'tech'] = 'E_SOLPVENDUSE_N_3'\n",
    "df_lifetime = pd.concat([df_lifetime, respv])\n",
    "\n",
    "respv = df_capfac.loc[df_capfac.loc[:,'tech'].str.contains('E_SOLPVCEN_N'),:]\n",
    "respv.loc[:,'tech'] = respv.loc[:,'tech'].str.replace('E_SOLPVCEN_N', 'E_SOLPVENDUSE_N')\n",
    "df_capfac = pd.concat([df_capfac, respv])\n",
    "\n",
    "respv = df_technology_new_cluster.loc[df_technology_new_cluster.loc[:,'tech'].str.contains('E_SOLPVCEN_N'),:]\n",
    "respv.loc[:,'tech'] = respv.loc[:,'tech'].str.replace('E_SOLPVCEN_N', 'E_SOLPVENDUSE_N')\n",
    "df_technology_new_cluster = pd.concat([df_technology_new_cluster, respv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add min residential PV generation to MinGenGroupWeight table\n",
    "df_target = pd.DataFrame()\n",
    "df_gen_respv = pd.read_csv('Residential_PV_gen_GWh.csv')\n",
    "if single_region==1:\n",
    "    df_gen_respv = df_gen_respv.groupby(['year']).sum().reset_index()\n",
    "    df_gen_respv['reg_names'] = 'US'\n",
    "for reg in df_gen_respv.reg_names.unique():\n",
    "    df_reg = df_gen_respv[df_gen_respv.reg_names==reg]\n",
    "    df_reg = df_reg[['year', 'reg_names', 'value']]\n",
    "    df_reg['reg_names'] = df_reg['reg_names'].str.replace(reg, reg+'_RESPVGRP')\n",
    "    df_reg['value'] *= 0.0036 #convert GWh to PJ\n",
    "    df_target = pd.concat([df_target, df_reg ])\n",
    "    \n",
    "df_target = df_target.rename(columns={'year':'periods','reg_names':'group_name','value':'min_act_g'})\n",
    "df_target.loc[:,'notes'] = 'From NREL dGen model mid PV cost scenario'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights_reg = pd.DataFrame()\n",
    "df_weights = pd.DataFrame()\n",
    "\n",
    "df_weights_reg.loc[:,'tech'] = ['E_SOLPVENDUSE_N_1', 'E_SOLPVENDUSE_N_2', 'E_SOLPVENDUSE_N_3' ]\n",
    "df_weights_reg.insert(0,'regions', 'US')\n",
    "df_weights_reg.loc[:,'group_name'] = ['', '', ''] \n",
    "df_weights_reg.loc[:,'act_fraction'] = [1., 1., 1.] \n",
    "\n",
    "for reg in df_gen_respv.reg_names.unique():\n",
    "    df_weights_reg.loc[:,'regions']= reg\n",
    "    df_weights_reg.loc[:,'group_name']= reg+'_RESPVGRP'\n",
    "    df_weights = pd.concat([df_weights, df_weights_reg])\n",
    "df_weights.loc[:,'tech_desc'] = 'All residential PV clusters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add capacity factor data for solar thermal technologies\n",
    "csp = df_capfac.loc[df_capfac.loc[:,'tech'].str.contains('E_SOLPVCEN_N_1'),:]\n",
    "csp.loc[:,'tech'] = csp.loc[:,'tech'].str.replace('E_SOLPVCEN_N_1', 'E_SOLTHCEN_N')\n",
    "csp = csp[(csp['regions']=='CA') | (csp['regions']=='SW')] #solar thermal only in CA and SW\n",
    "csp.loc[:,'cf_tech'] = 0.64\n",
    "df_capfac = pd.concat([df_capfac, csp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRITE TO DATABASE\n",
    "\n",
    "#CapacityToActivity\n",
    "df_c2a = df_c2a.drop_duplicates()\n",
    "df_table = df_c2a[~df_c2a['tech'].isin(old_df_c2a['tech'])]\n",
    "sqlite_table = 'CapacityToActivity'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#SegFrac\n",
    "if timeslice==0:\n",
    "    df_table = df_segfrac.drop_duplicates()\n",
    "    sqlite_table = 'SegFrac'\n",
    "    write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#CostInvest\n",
    "df_table = df_cost_invest.copy()\n",
    "df_table = df_table.loc[df_table.loc[:,'cost_invest']>0,:]\n",
    "sqlite_table = 'CostInvest'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#CostVariable\n",
    "df_table = df_cost_variable.drop_duplicates()\n",
    "sqlite_table = 'CostVariable'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#CostFixed\n",
    "df_table = df_cost_fixed.drop_duplicates()\n",
    "sqlite_table = 'CostFixed'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#Efficiency\n",
    "df_table = df_efficiency[['regions', 'input_comm','tech','vintage','output_comm', 'efficiency', 'eff_notes']].drop_duplicates()\n",
    "sqlite_table = 'Efficiency'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#tech_curtailment\n",
    "df_table = pd.DataFrame(df_efficiency.loc[['WND' in x or 'PV' in x or 'SOL' in x for x in df_efficiency.loc[:,'tech']],'tech'].drop_duplicates())\n",
    "df_table.insert(1,'notes','')\n",
    "sqlite_table = 'tech_curtailment'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "'''\n",
    "#tech_nonrenewable\n",
    "df_table = df_efficiency.loc[(df_efficiency.loc[:,'output_comm']=='ELCP') | (df_efficiency.loc[:,'output_comm']=='ELC') ,'tech'].drop_duplicates()\n",
    "sqlite_table = 'tech_nonrenewable'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#tech_renewable\n",
    "df_table = df_efficiency.loc[df_efficiency.loc[:,'output_comm']=='ELCP_Renewables','tech'].drop_duplicates()\n",
    "sqlite_table = 'tech_renewable'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "'''\n",
    "\n",
    "#tech_production in technologies table\n",
    "df_table = df_efficiency[['tech', 'flag', 'sector', 'tech_desc', 'tech_category']].drop_duplicates()\n",
    "df_table = df_table[~df_table['tech'].isin(tech_table['tech_technologies'])]\n",
    "sqlite_table = 'technologies'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#Ramping\n",
    "#ramping techs\n",
    "references = dict()\n",
    "references['technologies'] = 'tech'\n",
    "create_table(outputdB, 'tech_ramping', 'tech', 'tech', dict(),references)\n",
    "\n",
    "df_table = df_ramp.loc[:,['tech']].drop_duplicates()\n",
    "sqlite_table = 'tech_ramping'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#ramp up\n",
    "ramp_up_pairs = dict()\n",
    "ramp_up_pairs['regions'] = 'text'\n",
    "ramp_up_pairs['tech'] = 'text'\n",
    "ramp_up_pairs['ramp_up'] = 'real'\n",
    "references = dict()\n",
    "references['technologies'] = 'tech'\n",
    "create_table_mult_pkey(outputdB, 'RampUp', \"(regions, tech)\", 'tech', ramp_up_pairs,references)\n",
    "\n",
    "df_table = df_ramp.loc[:,['regions','tech', 'ramp_up']].drop_duplicates()\n",
    "sqlite_table = 'RampUp'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#ramp down\n",
    "ramp_up_pairs = dict()\n",
    "ramp_up_pairs['regions'] = 'text'\n",
    "ramp_up_pairs['tech'] = 'text'\n",
    "ramp_up_pairs['ramp_up'] = 'real'\n",
    "references = dict()\n",
    "references['technologies'] = 'tech'\n",
    "create_table_mult_pkey(outputdB, 'RampDown', \"(regions, tech)\", 'tech', ramp_up_pairs,references)\n",
    "\n",
    "df_table = df_ramp.loc[:,['regions','tech', 'ramp_down']].drop_duplicates()\n",
    "sqlite_table = 'RampDown'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#ExistingCapacity\n",
    "df_table = df_ex_cap.drop_duplicates()\n",
    "sqlite_table = 'ExistingCapacity'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#DiscountRate\n",
    "df_table = df_wacc.loc[:,['regions','tech', 'vintage', 'tech_rate', 'tech_rate_notes']].drop_duplicates()\n",
    "sqlite_table = 'DiscountRate'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#LifetimeLoanTech\n",
    "df_table = df_cap_rec_years.drop_duplicates()\n",
    "sqlite_table = 'LifetimeLoanTech'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#LifetimeTech\n",
    "conn = sqlite3.connect(outputdB)\n",
    "c = conn.cursor()\n",
    "df_lifetime_current = pd.read_sql_query(\"SELECT * FROM LifetimeTech\", conn)\n",
    "nonoverlap_tech = df_lifetime_current[~df_lifetime_current['tech'].isin(df_lifetime['tech'])]\n",
    "query = \"DELETE FROM LifetimeTech\"\n",
    "c.execute(query)\n",
    "conn.commit()\n",
    "conn.close()\n",
    "#df_lifetime.loc[:,'life']=120 #all to 120 years for now\n",
    "\n",
    "df_table = pd.concat([nonoverlap_tech, df_lifetime.drop_duplicates()])\n",
    "sqlite_table = 'LifetimeTech'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#CapacityCredit\n",
    "df_table = df_ccredit.loc[:,['regions','periods','tech', 'vintage','cf_tech','cf_tech_notes']].drop_duplicates()\n",
    "sqlite_table = 'CapacityCredit'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#tech_reserve\n",
    "df_table = df_ccredit.loc[:,['tech','cf_tech_notes']].drop_duplicates(subset='tech')\n",
    "df_table = df_table[~df_table['tech'].isin(tech_table['tech_technologies'])]\n",
    "df_table.rename(columns={'cf_tech_notes':'notes'}, inplace=True)\n",
    "sqlite_table = 'tech_reserve'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#CapacityFactorTech\n",
    "df_table = df_capfac.drop_duplicates()\n",
    "sqlite_table = 'CapacityFactorTech'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#MaxCapacity\n",
    "df_table = df_maxcap.loc[:,['regions','periods','tech', 'maxcap','maxcap_units','maxcap_tech_notes']].drop_duplicates()\n",
    "sqlite_table = 'MaxCapacity'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#DSD\n",
    "if timeslice==0:\n",
    "    df_table = df_load.loc[:,['regions','season_name','time_of_day_name','demand_name','dds','dds_notes']]\n",
    "    sqlite_table = 'DemandSpecificDistribution'\n",
    "    write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "    #time_season\n",
    "    df_table = pd.DataFrame(df_capfac.loc[:,'season_name'].drop_duplicates())\n",
    "    df_table.rename(columns={'season_name':'t_season'},inplace=True)\n",
    "    sqlite_table = 'time_season'\n",
    "    write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "    #time_of_day\n",
    "    df_table = pd.DataFrame(df_capfac.loc[:,'time_of_day_name'].drop_duplicates())\n",
    "    df_table.rename(columns={'time_of_day_name':'t_day'},inplace=True)\n",
    "    sqlite_table = 'time_of_day'\n",
    "    write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#time_periods\n",
    "df_table = pd.DataFrame(columns=['t_periods','flag'])\n",
    "#df_table.loc[:,'t_periods'] = np.sort(df_costs.loc[:,'vintage'].unique().astype(int))\n",
    "df_table.loc[:,'t_periods'] = np.hstack((np.arange(df_costs.loc[:,'vintage'].astype(int).min()-1,2020), \n",
    "                                         np.arange(2020,2056,5)))\n",
    "df_table.loc[:,'flag'] = ['e' if x <2020  else 'f' for x in df_table.loc[:,'t_periods']]\n",
    "sqlite_table = 'time_periods'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#MinGenGroupWeight\n",
    "df_table = df_weights\n",
    "sqlite_table = 'MinGenGroupWeight'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#MinGenGroupTarget\n",
    "df_table = df_target\n",
    "sqlite_table = 'MinGenGroupTarget'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#groups\n",
    "df_table = pd.DataFrame(columns=['group_name', 'notes'])\n",
    "df_table.loc[:,'group_name'] = df_weights['group_name'].unique()\n",
    "sqlite_table = 'groups'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#tech_groups\n",
    "df_table = pd.DataFrame(columns=['tech', 'notes'])\n",
    "df_table.loc[:,'tech'] = df_weights['tech'].unique()\n",
    "sqlite_table = 'tech_groups'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "if len(settings['region_aggregations'])>1 & ('E_TRANS_R' in df_efficiency.tech.unique()):\n",
    "    df_table = pd.DataFrame(columns = ['tech', 'notes'])\n",
    "    df_table.loc[0] = ['E_TRANS_R','#existing electric transmission']\n",
    "    df_table.loc[1] = ['E_TRANS_N','#new electric transmission']\n",
    "    sqlite_table = 'tech_exchange'\n",
    "    write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#write CO2, nox_elc and so2_elc emissions for CCS technologies to EmissionActivity table\n",
    "df_table = pd.concat([df_new_emiss, ccs_techs])\n",
    "df_table = df_table[df_emiss.columns]\n",
    "sqlite_table = 'EmissionActivity'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#write techoutputsplit\n",
    "df_table = df_tosplit_ccs\n",
    "sqlite_table = 'TechOutputSplit'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#new table tech_new_cluster\n",
    "conn = sqlite3.connect(outputdB)\n",
    "c = conn.cursor()\n",
    "c.execute(\"CREATE TABLE IF NOT EXISTS tech_new_cluster ( regions TEXT, tech TEXT, cap_size REAL, PRIMARY KEY (regions, tech))\")\n",
    "conn.commit()    \n",
    "conn.close()\n",
    "sqlite_table = 'tech_new_cluster'\n",
    "write_sql(df_technology_new_cluster.drop_duplicates(), sqlite_table, outputdB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map regions to descriptions\n",
    "if single_region!=1:\n",
    "    map_region_desc = dict()\n",
    "    map_region_desc['CA'] = 'California'\n",
    "    map_region_desc['NW'] = 'Northwestern US'\n",
    "    map_region_desc['SW'] =  'Southwestern US'\n",
    "    map_region_desc['TX'] =  'Texas'\n",
    "    map_region_desc['N_CEN'] =  'North Central US'\n",
    "    map_region_desc['CEN'] =  'Central US'\n",
    "    map_region_desc['SE'] =  'Southeastern US'\n",
    "    map_region_desc['MID_AT'] =  'Mid Atlantic US'\n",
    "    map_region_desc['NE'] = 'Northeastern US'\n",
    "else:\n",
    "    map_region_desc = dict()\n",
    "    map_region_desc['US'] = 'United States'\n",
    "\n",
    "df_regions = pd.DataFrame.from_dict(map_region_desc, orient='index').reset_index()\n",
    "df_regions.columns = ['regions', 'region_note']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(outputdB)\n",
    "c = conn.cursor()\n",
    "\n",
    "if elec_only==1:\n",
    "    demands = elec_demand.groupby('periods').sum().reset_index()\n",
    "    demands = demands.melt(id_vars='periods')\n",
    "    demands.rename(columns={'region':'regions', 'value':'demand'}, inplace=True)\n",
    "    demands.insert(2,'demand_comm', 'ELC_dem')\n",
    "    demands.loc[:,'demand'] *=3.6e-6 #convert MWh to PJ\n",
    "    demands.loc[:,'demand_units'] = 'PJ'\n",
    "    demands.loc[:,'demand_notes'] = 'from Power Genome: ' + scenario\n",
    "    df_table = demands[['regions','periods','demand_comm','demand','demand_units','demand_notes']]\n",
    "    df_table.to_sql('Demand',conn, if_exists='append', index=False)\n",
    "    \n",
    "    df_elc_comm = pd.DataFrame(columns=['comm_name', 'flag','comm_desc'], data=[['ELC_dem','d','Final electricity load']])\n",
    "    df_elc_comm.to_sql('commodities',conn, if_exists='append', index=False)\n",
    "    \n",
    "    df_elc_tech = pd.DataFrame(columns=['tech', 'flag','sector','tech_desc','tech_category'], data=[['ELC_BLND','p', 'electric','dummy technology to meet electricity demand commodity', '']])\n",
    "    df_elc_tech.to_sql('technologies',conn, if_exists='append', index=False)\n",
    "    \n",
    "    df_elc_tech = pd.DataFrame(columns=['regions', 'input_comm', 'tech', 'vintage','output_comm', 'efficiency', 'eff_notes'])\n",
    "    df_elc_tech.loc[:,'regions'] = df_regions['regions'].unique()\n",
    "    df_elc_tech.loc[:,'input_comm'] = 'ELC'\n",
    "    df_elc_tech.loc[:,'tech'] = 'ELC_BLND'\n",
    "    df_elc_tech.loc[:,'output_comm'] = 'ELC_dem'\n",
    "    df_elc_tech.loc[:,'efficiency'] = 1\n",
    "    df_elc_tech.loc[:,'vintage'] = start_year\n",
    "    df_elc_tech.loc[:,'eff_notes'] = 'dummy technology to meet electricity demand commodity'\n",
    "    df_elc_tech.to_sql('Efficiency',conn, if_exists='append', index=False)\n",
    "    \n",
    "    query = \"UPDATE Efficiency SET output_comm='ELC' WHERE tech LIKE 'E_SOLPVENDUSE%'\"\n",
    "    c.execute(query)\n",
    "\n",
    "iterval = 0\n",
    "while len(c.execute(\"SELECT * FROM Efficiency WHERE output_comm NOT IN (SELECT input_comm FROM Efficiency)\\\n",
    "                       AND output_comm NOT IN (SELECT comm_name FROM commodities WHERE flag='d');\").fetchall()) > 0:\n",
    "\n",
    "    c.execute(\"DELETE FROM Efficiency WHERE output_comm NOT IN (SELECT input_comm FROM Efficiency) \\\n",
    "             AND output_comm NOT IN (SELECT comm_name FROM commodities WHERE flag='d');\")\n",
    "    iterval+=1\n",
    "    if iterval>10:\n",
    "        break\n",
    "print(iterval)\n",
    "\n",
    "for table in table_list:\n",
    "    if table[0] != 'Efficiency':  \n",
    "        df_cols = c.execute(\"SELECT * FROM pragma_table_info('\" + table[0] + \"')\").fetchall()\n",
    "        df_cols = [x[1] for x in df_cols]\n",
    "        if 'tech' in df_cols:\n",
    "            c.execute(\"UPDATE \"+str(table[0])+\" SET tech = TRIM(tech, CHAR(37,13,10));\")\n",
    "            try:\n",
    "                if 'vintage' in df_cols:\n",
    "                    # If t doesn't exist in Efficiency table after the deletions made above, \n",
    "                    # it is deleted from other tables.\n",
    "                    c.execute(\"DELETE FROM \"+str(table[0])+\" WHERE tech || vintage \\\n",
    "                        NOT IN (SELECT tech || vintage FROM Efficiency)\")\n",
    "                else:\n",
    "                    c.execute(\"DELETE FROM \"+str(table[0])+\" WHERE tech NOT IN (SELECT tech FROM Efficiency);\")\n",
    "            except:\n",
    "                print(table[0])\n",
    "        if table[0] == 'EmissionActivity':\n",
    "            c.execute(\"DELETE FROM EmissionActivity WHERE input_comm || tech || vintage || output_comm \\\n",
    "                        NOT IN (SELECT input_comm || tech || vintage || output_comm FROM Efficiency)\")\n",
    "\n",
    "\n",
    "c.execute(\"UPDATE commodities SET comm_name = TRIM(comm_name, CHAR(10,13,37))\")\n",
    "    # delete unused commodities otherwise the model throws an error\n",
    "c.execute(\"DELETE FROM commodities WHERE flag!='e' AND comm_name NOT IN (SELECT input_comm from Efficiency UNION SELECT output_comm from Efficiency);\")\n",
    "\n",
    "\n",
    "\n",
    "if (single_region!=1) & (elec_only!=1) :\n",
    "    #update regional transportation demands \n",
    "    df_transport = pd.read_csv('../TransportationSector/NREL_EFS/transportation_regional_demands_adjusted.csv')\n",
    "    df_transport = df_transport[['regions', 'periods', 'demand_comm', 'demand','demand_units','demand_notes']]\n",
    "    query = \"DELETE FROM Demand WHERE demand_units LIKE '%vmt%' \\\n",
    "    OR demand_units LIKE '%mile%' OR demand_units LIKE '%bpm%'OR demand_units LIKE '%btm%'\"\n",
    "    c.execute(query)\n",
    "    df_transport.to_sql('Demand',conn, if_exists='append', index=False)\n",
    "\n",
    "    #update regional building demands \n",
    "    df_buildings = pd.read_csv('../BuildingsSector/NREL_EFS/buildings_regional_demands_adjusted.csv')\n",
    "    df_buildings = df_buildings[['regions', 'periods', 'demand_comm', 'demand','demand_units','demand_notes']]\n",
    "    query = \"DELETE FROM Demand WHERE \\\n",
    "    substr(demand_comm,1,1) = 'C'OR substr(demand_comm,1,1) = 'R'\"\n",
    "    c.execute(query)\n",
    "    df_buildings.to_sql('Demand',conn, if_exists='append', index=False)\n",
    "\n",
    "    #update regional industry demands \n",
    "    df_industry = pd.read_csv('../IndustrialSector/NREL_EFS/industry_regional_demands_adjusted.csv')\n",
    "    df_industry = df_industry[['regions', 'periods', 'demand_comm', 'demand','demand_units','demand_notes']]\n",
    "    query = \"DELETE FROM Demand  \\\n",
    "    WHERE demand_comm LIKE '%IND%'\"\n",
    "    c.execute(query)\n",
    "    df_industry.to_sql('Demand',conn, if_exists='append', index=False)\n",
    "\n",
    "    #find remaining demands at US level, divide by 9 to get demands in each region\n",
    "    df_remains = pd.read_sql_query(\"SELECT * FROM Demand WHERE regions='US'\", conn)\n",
    "    query = \"DELETE FROM Demand WHERE regions='US'\"\n",
    "    c.execute(query)\n",
    "    for reg in df_industry.regions.unique():\n",
    "        df_remains_new = df_remains.copy()\n",
    "        df_remains_new.regions=reg\n",
    "        df_remains_new['demand'] /=9\n",
    "        df_remains_new.to_sql('Demand',conn, if_exists='append', index=False)\n",
    "\n",
    "    for table in table_list:\n",
    "        df_cols = c.execute(\"SELECT * FROM pragma_table_info('\" + table[0] + \"')\").fetchall()\n",
    "        df_cols = [x[1] for x in df_cols]\n",
    "\n",
    "        if table[0]=='regions':\n",
    "            query = \"DELETE FROM \" + table[0]\n",
    "            c.execute(query)\n",
    "            df_regions.to_sql(table[0],conn, if_exists='append', index=False)\n",
    "        elif 'regions' in df_cols:\n",
    "            df_remains = pd.read_sql_query(\"SELECT * FROM \" + table[0] + \" WHERE regions='US'\", conn)\n",
    "            query = \"DELETE FROM \" + table[0] + \" WHERE regions='US'\"\n",
    "            c.execute(query)\n",
    "            for reg in df_regions.regions.unique():\n",
    "                df_remains_new = df_remains.copy()\n",
    "                df_remains_new.regions=reg\n",
    "                if table[0] in ['ExistingCapacity']:\n",
    "                    df_remains_new['exist_cap'] /=9\n",
    "                #if table[0] in ['GrowthRateMax']:\n",
    "                #    df_remains_new['growthrate_max'] /=9\n",
    "                #if table[0] in ['GrowthRateSeed']:\n",
    "                #    df_remains_new['growthrate_seed'] /=9\n",
    "                if table[0] in ['MinActivity']:\n",
    "                    df_remains_new['minact'] /=20\n",
    "                df_remains_new.to_sql(table[0],conn, if_exists='append', index=False)\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con=sqlite3.connect(outputdB, isolation_level=None)\n",
    "con.execute(\"VACUUM\")\n",
    "con.commit()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(outputdB)\n",
    "with open(outputdB.replace('ite',''), 'w') as f:\n",
    "    for line in conn.iterdump():\n",
    "        '''\n",
    "        if 'e+' in line or 'e-0' in line:\n",
    "            val = [x for x in line.split(',') if 'e+' in x or 'e-0' in x][0]\n",
    "            ls = line.split(',')\n",
    "            ind = ls.index(val)\n",
    "            ls[ind] = [str(round(float(x),2)) for x in line.split(',') if 'e+' in x or 'e-0' in x ][0]\n",
    "            line = ','.join(ls)\n",
    "        '''\n",
    "        if 'CREATE TABLE' in line:\n",
    "            f.write('\\n' )\n",
    "        f.write('%s\\n' % line)\n",
    "        \n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
