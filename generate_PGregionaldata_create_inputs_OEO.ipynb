{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from powergenome.generators import GeneratorClusters, add_genx_model_tags\n",
    "from powergenome.GenX import reduce_time_domain\n",
    "from powergenome.load_profiles import make_final_load_curves\n",
    "from powergenome.params import DATA_PATHS\n",
    "from powergenome.util import (\n",
    "    build_scenario_settings,\n",
    "    init_pudl_connection,\n",
    "    load_settings,\n",
    ")\n",
    "from powergenome.external_data import (\n",
    "    make_demand_response_profiles,\n",
    "    make_generator_variability,\n",
    ")\n",
    "\n",
    "from powergenome.load_profiles import (\n",
    "    make_load_curves, \n",
    "    add_load_growth, \n",
    "    make_final_load_curves, \n",
    "    make_distributed_gen_profiles,\n",
    ")\n",
    "from powergenome.external_data import make_demand_response_profiles\n",
    "from powergenome.generators import GeneratorClusters\n",
    "from powergenome.util import (\n",
    "    build_scenario_settings,\n",
    "    init_pudl_connection,\n",
    "    load_settings,\n",
    "    reverse_dict_of_lists,\n",
    "    remove_feb_29\n",
    ")\n",
    "\n",
    "from powergenome.load_profiles import make_final_load_curves\n",
    "from powergenome.generators import GeneratorClusters\n",
    "from powergenome.util import (\n",
    "    build_scenario_settings,\n",
    "    init_pudl_connection,\n",
    "    load_settings,\n",
    "    reverse_dict_of_lists\n",
    ")\n",
    "\n",
    "from powergenome.GenX import reduce_time_domain, add_misc_gen_values\n",
    "from powergenome.external_data import make_generator_variability\n",
    "\n",
    "from powergenome.generators import load_ipm_shapefile\n",
    "from powergenome.GenX import (\n",
    "    network_line_loss,\n",
    "    network_max_reinforcement,\n",
    "    network_reinforcement_cost,\n",
    ")\n",
    "from powergenome.transmission import (\n",
    "    agg_transmission_constraints,\n",
    "    transmission_line_distance,\n",
    ")\n",
    "from powergenome.util import init_pudl_connection, load_settings\n",
    "\n",
    "pd.options.display.max_columns = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sqlite3\n",
    "import shutil\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_system = 0\n",
    "if example_system ==1:\n",
    "    run_folder = \"example_system\" \n",
    "    settings_file = \"test_settings_mod.yml\" \n",
    "    scenario = \"p1\"\n",
    "else:\n",
    "    run_folder = \"US_Regional\"\n",
    "    settings_file = \"Temoa_settings.yml\"\n",
    "    scenario = \"p6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create New SQL File\n",
    "emptydB = '/Users/aranyavenkatesh/Documents/EnergyOutlook/temoa/data_files/US_National.sqlite'\n",
    "#outfilename_w_ext = outFilename + '.sqlite'\n",
    "if example_system==1:\n",
    "    outputdB = '/Users/aranyavenkatesh/Documents/EnergyOutlook/temoa/data_files/CA_twozone_test.sqlite'\n",
    "else:\n",
    "    outputdB = '/Users/aranyavenkatesh/Documents/EnergyOutlook/temoa/data_files/US_Regional_push.sqlite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pudl_engine, pudl_out = init_pudl_connection()\n",
    "cwd = Path.cwd()\n",
    "\n",
    "settings_path = (\n",
    "    cwd / run_folder / settings_file\n",
    ")\n",
    "settings = load_settings(settings_path)\n",
    "settings[\"input_folder\"] = settings_path.parent / settings[\"input_folder\"]\n",
    "scenario_definitions = pd.read_csv(\n",
    "    settings[\"input_folder\"] / settings[\"scenario_definitions_fn\"]\n",
    ")\n",
    "scenario_settings = build_scenario_settings(settings, scenario_definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_periods = list(scenario_settings.keys())\n",
    "start_year = all_periods[0]\n",
    "run_new = 1\n",
    "file_prefix = str(settings_path).replace('.yml','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_new==1:\n",
    "    new_gen = pd.DataFrame()\n",
    "    for year in all_periods[1:]: #new gen for start year is included in all_gen\n",
    "        gc = GeneratorClusters(pudl_engine, pudl_out, scenario_settings[year][scenario])\n",
    "        new_gen_year = gc.create_new_generators()\n",
    "        new_gen_year.loc[:,'operating_year'] = year\n",
    "        new_gen = pd.concat([new_gen, new_gen_year]) #create new generators for periods beyond the first\n",
    "    load_curves = make_final_load_curves(pudl_engine, scenario_settings[start_year][scenario])\n",
    "\n",
    "\n",
    "\n",
    "    gc = GeneratorClusters(pudl_engine, pudl_out, scenario_settings[start_year][scenario])\n",
    "    all_gens = gc.create_all_generators() #create existing and new generators for the first time period\n",
    "\n",
    "    #add misc_values from misc_gen_inputs_fn file in extra_inputs folder\n",
    "    all_gens = add_misc_gen_values(all_gens,settings)\n",
    "    new_gen = add_misc_gen_values(new_gen,settings)\n",
    "\n",
    "    gen_variability = make_generator_variability(all_gens)\n",
    "\n",
    "    (\n",
    "        reduced_resource_profile,\n",
    "        reduced_load_profile,\n",
    "        long_duration_storage,\n",
    "    ) = reduce_time_domain(gen_variability, load_curves, scenario_settings[start_year][scenario])\n",
    "\n",
    "    if len(settings['region_aggregations'])>1:\n",
    "        transmission = agg_transmission_constraints(pudl_engine=pudl_engine, settings=settings)\n",
    "        model_regions_gdf = load_ipm_shapefile(settings)\n",
    "        transmission = transmission_line_distance(\n",
    "            trans_constraints_df=transmission,\n",
    "            ipm_shapefile=model_regions_gdf,\n",
    "            settings=settings,\n",
    "        )\n",
    "        transmission = network_line_loss(transmission=transmission, settings=settings)\n",
    "        transmission = network_reinforcement_cost(transmission=transmission, settings=settings)\n",
    "        transmission = network_max_reinforcement(transmission=transmission, settings=settings)\n",
    "        transmission.to_csv(file_prefix + 'transmission.csv', index=False)\n",
    "\n",
    "    gen_variability.to_csv(file_prefix + 'gen_variability.csv', index=False)\n",
    "    load_curves.to_csv(file_prefix + 'load_curves.csv', index=False)\n",
    "    reduced_load_profile.to_csv(file_prefix + 'reduced_load_profile.csv', index=False)\n",
    "    reduced_resource_profile.to_csv(file_prefix + 'reduced_resource_profile.csv', index=False)\n",
    "    all_gens.to_csv(file_prefix + 'all_gens.csv', index=False)\n",
    "    new_gen.to_csv(file_prefix + 'new_gen.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_new!=1:\n",
    "    #reduced_load_profile = pd.read_csv(file_prefix + 'reduced_load_profile.csv')\n",
    "    #reduced_resource_profile = pd.read_csv(file_prefix + 'reduced_resource_profile.csv')\n",
    "    all_gens = pd.read_csv(file_prefix + 'all_gens.csv')\n",
    "    new_gen = pd.read_csv(file_prefix + 'new_gen.csv')\n",
    "    \n",
    "    gen_variability = pd.read_csv(file_prefix + 'gen_variability.csv')\n",
    "    load_curves = pd.read_csv(file_prefix + 'load_curves.csv')\n",
    "    \n",
    "    \n",
    "    (\n",
    "        reduced_resource_profile,\n",
    "        reduced_load_profile,\n",
    "        long_duration_storage,\n",
    "    ) = reduce_time_domain(gen_variability, load_curves, scenario_settings[start_year][scenario])\n",
    "    \n",
    "    if len(settings['region_aggregations'])>1:\n",
    "        transmission = pd.read_csv(file_prefix + 'transmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_tech_names = dict()\n",
    "map_tech_names['biomass'] = 'E_BIO_R'\n",
    "map_tech_names['conventional_hydroelectric'] = 'E_HYDCONV_R'\n",
    "map_tech_names['conventional_steam_coal'] = 'E_COALSTM_R'\n",
    "map_tech_names['natural_gas_fired_combined_cycle'] = 'E_NGACC_R'\n",
    "map_tech_names['natural_gas_fired_combustion_turbine'] = 'E_NGACT_R'\n",
    "map_tech_names['natural_gas_steam_turbine'] = 'E_NGASTM_R'\n",
    "map_tech_names['nuclear'] = 'E_URNLWR_R'\n",
    "map_tech_names['onshore_wind_turbine'] = 'E_WND_R'\n",
    "map_tech_names['small_hydroelectric'] = 'E_HYDSM_R'\n",
    "map_tech_names['solar_photovoltaic'] = 'E_SOLPV_R'\n",
    "map_tech_names['hydroelectric_pumped_storage'] = 'E_HYDPS_R'\n",
    "map_tech_names['geothermal'] = 'E_GEO_R'\n",
    "map_tech_names['naturalgas_ccccsavgcf_mid'] = 'E_NGACC_CCS_N'\n",
    "map_tech_names['naturalgas_ccavgcf_mid'] = 'E_NGAACC_N'\n",
    "map_tech_names['naturalgas_ctavgcf_mid'] = 'E_NGAACT_N'\n",
    "map_tech_names['landbasedwind_ltrg1_mid_110'] = 'E_WND_N'\n",
    "map_tech_names['utilitypv_losangeles_mid_250_0_2'] = 'E_SOLPVCEN_N'\n",
    "map_tech_names['naturalgas_ccs100_mid'] = 'E_NGACC_CCS_ZERO_N'\n",
    "map_tech_names['nuclear_mid'] = 'E_URNLWR_N'\n",
    "map_tech_names['battery_mid'] = 'E_Batt'\n",
    "map_tech_names['ev_load_shifting'] = 'E_LD_SHFT'\n",
    "map_tech_names['offshorewind_otrg10_mid_floating_1_250'] = 'E_OFWND_N'\n",
    "map_tech_names['geothermal_hydroflash_mid'] = 'E_GEOF_N'\n",
    "map_tech_names['geothermal_hydrobinary_mid'] = 'E_GEOB_N'\n",
    "map_tech_names['csp_class5_10hourstes'] = 'E_SOLTHCEN_N'\n",
    "map_tech_names['biopower_dedicated_mid'] = 'E_BIO_N'\n",
    "map_tech_names['coal_igccavgcf_mid'] = 'E_COALIGCC_N'\n",
    "map_tech_names['coal_ccs90avgcf_mid'] = 'E_COALUSC_90CCS_N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_tech_desc = dict()\n",
    "map_tech_desc['E_BIO_R'] = '#existing bio-energy'\n",
    "map_tech_desc['E_BIO_N'] = '#new bio-energy'\n",
    "map_tech_desc['E_HYDSM_R'] = '#existing small hydroelectric power plant'\n",
    "map_tech_desc['E_HYDPS_R'] = '#existing pumped hydro storage'\n",
    "map_tech_desc['E_WND_N'] = '#new wind power plant'\n",
    "map_tech_desc['E_NGACC_CCS_ZERO_N'] = '#new natural gas combined cycle with 100% CCS power plant'\n",
    "map_tech_desc['E_OFWND_N'] = '#new offshore wind, floating'\n",
    "map_tech_desc['E_GEOF_N'] = '#new geothermal, hydro flash'\n",
    "map_tech_desc['E_GEOB_N'] = '#new geothermal, hydro binary'\n",
    "map_tech_desc['E_COALUSC_90CCS_N'] = '#new ultrasupercritical pulverized coal with 90% CCS power plant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map PG technology names to OEO names\n",
    "all_gens['Resource'] = all_gens['Resource'].map(map_tech_names)\n",
    "new_gen['Resource'] = new_gen['Resource'].map(map_tech_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gens[all_gens.technology.str.contains('Geo')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiply capex values by the regional multipliers, since this is not done in PowerGenome\n",
    "new_gen['capex'] *= new_gen['regional_cost_multiplier']\n",
    "all_gens['capex'] *= all_gens['regional_cost_multiplier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(emptydB)\n",
    "c = conn.cursor()\n",
    "tech_table = pd.read_sql_query(\"SELECT * FROM technologies\", conn)\n",
    "tech_table.rename(columns={'tech':'tech_technologies'}, inplace=True)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_count = all_gens[['technology', 'cluster']].groupby('technology').max().reset_index()\n",
    "cluster_count.columns = ['technology', 'max_cluster']\n",
    "all_gens = all_gens.merge(cluster_count, on='technology', how='left')\n",
    "\n",
    "mask = all_gens['max_cluster']<=1\n",
    "all_gens.loc[mask, 'tech'] =  all_gens.loc[mask, 'Resource'] + '-' + all_gens.loc[mask,'region']\n",
    "all_gens.loc[~mask, 'tech'] =  all_gens.loc[~mask, 'Resource'] + '-' + all_gens.loc[~mask,'region'] + \\\n",
    "'-' +all_gens.loc[~mask,'cluster'].map(int).map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_count = new_gen[['technology', 'cluster']].groupby('technology').max().reset_index()\n",
    "cluster_count.columns = ['technology', 'max_cluster']\n",
    "new_gen = new_gen.merge(cluster_count, on='technology', how='left')\n",
    "\n",
    "mask = new_gen['max_cluster']<=1\n",
    "new_gen.loc[mask, 'tech'] =  new_gen.loc[mask, 'Resource'] + '-' + new_gen.loc[mask,'region']\n",
    "new_gen.loc[~mask, 'tech'] =  new_gen.loc[~mask, 'Resource'] + '-' + new_gen.loc[~mask,'region'] + \\\n",
    "'-' +new_gen.loc[~mask,'cluster'].map(int).map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat start year gens with remaining new gens\n",
    "all_gens_multi_year = pd.concat([all_gens, new_gen])\n",
    "\n",
    "all_gens_multi_year = all_gens_multi_year.merge(tech_table, left_on = ['Resource'], right_on =['tech_technologies'], how='left')\n",
    "all_gens_multi_year['flag'].fillna('p', inplace=True)\n",
    "all_gens_multi_year['sector'].fillna('electric', inplace=True)\n",
    "all_gens_multi_year['tech_desc'].fillna(all_gens_multi_year['Resource'].map(map_tech_desc), inplace=True)\n",
    "\n",
    "all_gens_multi_year.loc[all_gens_multi_year['Resource']=='E_HYDPS_R','flag'] = 'ps'\n",
    "\n",
    "#all_gens_multi_year = all_gens_multi_year[all_gens.columns]\n",
    "all_gens_multi_year.loc[np.isnan(all_gens_multi_year.operating_year),'operating_year'] = all_periods[0]\n",
    "all_gens_multi_year.loc[all_gens_multi_year.operating_year==0,'operating_year'] = all_periods[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#account for storage (batteries) capex and FOM for a specific storage duration\n",
    "stor_dur = 4\n",
    "mask = all_gens_multi_year['tech'].str.contains('Batt')\n",
    "orig_batt = all_gens_multi_year.loc[mask,:].copy()\n",
    "orig_batt['Heat_rate_MMBTU_per_MWh'] = 3412/0.85 #assuming 85% efficiency\n",
    "all_gens_multi_year.loc[mask,'capex'] += stor_dur*all_gens_multi_year.loc[mask,'capex_mwh']\n",
    "all_gens_multi_year.loc[mask,'Fixed_OM_cost_per_MWyr'] = 2.5/100*all_gens_multi_year.loc[mask,'capex_mwh']\n",
    "stor_dur = 8\n",
    "orig_batt.loc[mask,'capex'] += stor_dur*orig_batt.loc[mask,'capex_mwh']\n",
    "orig_batt.loc[mask,'Fixed_OM_cost_per_MWyr'] = 2.5/100*orig_batt.loc[mask,'capex_mwh']\n",
    "orig_batt['Resource'] = 'E_Batt8hr'\n",
    "orig_batt['tech'] = orig_batt['tech'].str.replace('E_Batt','E_Batt8hr')\n",
    "orig_batt['tech_technologies'] = orig_batt['tech_technologies'].str.replace('E_Batt','E_Batt8hr')\n",
    "orig_batt['tech_desc'] = orig_batt['tech_desc'].str.replace('storage','storage (8 hour)')\n",
    "\n",
    "all_gens_multi_year = pd.concat([all_gens_multi_year, orig_batt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_region(df_tech):\n",
    "    return [x[1] for x in df_tech.str.split('-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_tech(df_tech):\n",
    "    output_vals = []\n",
    "    for val in df_tech:\n",
    "        try:\n",
    "            o_val = val.split('-')[0] + '_' + val.split('-')[2]\n",
    "        except:\n",
    "            o_val = val.split('-')[0]\n",
    "        output_vals.append(o_val)\n",
    "    return output_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sql(df_table, sqlite_table, outputdB):\n",
    "#code snippets from https://github.com/EnergyModels/temoatools/tree/master/temoatools\n",
    "\n",
    "    df_table = pd.DataFrame(df_table)\n",
    "    \n",
    "    # Set-up sqlite connection\n",
    "    conn = sqlite3.connect(outputdB)\n",
    "    c = conn.cursor()\n",
    "\n",
    "    #----------\n",
    "    # sqlite file prep\n",
    "    #----------\n",
    "\n",
    "    # Create SQL command based on number of entries\n",
    "    command = 'INSERT INTO ' + sqlite_table + ' VALUES (?'\n",
    "    for i in range(len(df_table.columns)-1):\n",
    "        command = command + ',?'\n",
    "    command = command + ')'\n",
    "\n",
    "    # Execute SQL command\n",
    "    try:\n",
    "        c.executemany(command,np.array(df_table))\n",
    "    except:\n",
    "        print(command)\n",
    "        print(np.array(df_table))\n",
    "        c.executemany(command, np.array(df_table))\n",
    "\n",
    "    #----------\n",
    "    # Save(commit) the changes and close sqlite file\n",
    "    #----------\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regions_list = all_gens_multi_year.region.unique()\n",
    "\n",
    "# Delete old *.sqlite file (if it already exists) and copy/rename copy of temoa_schema.sqlite\n",
    "if os.path.isfile(outputdB):\n",
    "    os.remove(outputdB)\n",
    "shutil.copyfile(emptydB, outputdB)\n",
    "##remove data from tables\n",
    "conn = sqlite3.connect(outputdB)\n",
    "c = conn.cursor()\n",
    "\n",
    "table_list = c.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'Output%'\").fetchall()\n",
    "c.execute(\"UPDATE Efficiency SET tech = TRIM(tech);\") #trim spaces. Need to trim carriage return\n",
    "\n",
    "#remove entire table\n",
    "for table in ['CapacityToActivity','SegFrac', 'DemandSpecificDistribution', #'LifetimeProcess', \n",
    "              'CapacityCredit', 'CapacityFactorTech', 'MinGenGroupWeight','MinGenGroupTarget', #'MinCapacity','StorageDuration',\n",
    "              'GrowthRateMax','GrowthRateSeed', #'MinActivity',\n",
    "              'time_of_day', 'time_season', 'time_periods', 'tech_nonrenewable', 'tech_renewable',\n",
    "              'Output_CapacityByPeriodAndTech','Output_V_Capacity','Output_VFlow_In', 'Output_VFlow_Out',\n",
    "              'Output_Objective', 'Output_Emissions', 'Output_curtailment', 'Output_Costs',\n",
    "              'EmissionLimit'\n",
    "              ]:\n",
    "    query = \"\"\"DELETE FROM \"\"\" + table\n",
    "    c.execute(query)\n",
    "\n",
    "#delete distributed generation (from solar) from Efficiency table in original database\n",
    "query = \"\"\"DELETE FROM Efficiency WHERE input_comm='ELCDIST_R'\"\"\"\n",
    "c.execute(query)\n",
    "query = \"\"\"DELETE FROM Efficiency WHERE tech='IMPELC'\"\"\"\n",
    "c.execute(query)\n",
    "\n",
    "techs_keep = ['E_BECCS_N','H2_STO150', 'E_H2CC_N']\n",
    "for tech_keep in techs_keep:\n",
    "    #modify the technologies that are retained from US_National for multiple regions\n",
    "    for table in table_list:\n",
    "        df_cols = c.execute(\"SELECT * FROM pragma_table_info('\" + table[0] + \"')\").fetchall()\n",
    "        df_cols = [x[1] for x in df_cols]\n",
    "        if 'tech' in df_cols:\n",
    "            df = pd.read_sql_query(\"SELECT * FROM \" + table[0] + \" WHERE tech='\" + tech_keep + \"'\", conn)\n",
    "            if table[0]=='CapacityFactorTech':\n",
    "                query = \"DELETE FROM \" + table[0] + \" WHERE tech='\" + tech_keep + \"'\"\n",
    "                c.execute(query)\n",
    "            elif (len(df)>0) & ('regions' in df.columns):\n",
    "                query = \"DELETE FROM \" + table[0] + \" WHERE tech='\" + tech_keep + \"'\"\n",
    "                c.execute(query)\n",
    "                for reg in regions_list:\n",
    "                    df_new = df.copy()\n",
    "                    df_new['regions'] = reg                        \n",
    "                    df_new.to_sql(table[0],conn, if_exists='append', index=False)\n",
    "\n",
    "#modify efficiency for processes that meet demands that can only be met by processes of vintage 2017\n",
    "for table in ['Efficiency','EmissionActivity']:\n",
    "    query = \"\"\"UPDATE \"\"\" + table + \"\"\" SET vintage=2020\n",
    "    WHERE vintage=2017\n",
    "    AND regions || input_comm || tech || output_comm NOT IN \n",
    "    (\n",
    "        SELECT DISTINCT\n",
    "        regions || input_comm || tech || output_comm\n",
    "        FROM \"\"\" + table + \"\"\" WHERE vintage = 2020\n",
    "    )\"\"\"\n",
    "    c.execute(query)\n",
    "\n",
    "#modify cost tables to account for these processes\n",
    "for table in ['CostFixed','CostVariable']:\n",
    "    query= \"\"\"UPDATE \"\"\" + table + \"\"\" SET vintage=2020\n",
    "        WHERE \n",
    "          vintage = 2017 \n",
    "          AND\n",
    "          regions || tech NOT IN \n",
    "        (SELECT DISTINCT\n",
    "          regions || tech\n",
    "        FROM \"\"\" + table + \"\"\" WHERE vintage = 2020)\"\"\"\n",
    "    c.execute(query)\n",
    "\n",
    "#delete all elec technologies except for those in techs_keep\n",
    "for table in ['Efficiency', 'ExistingCapacity', 'DiscountRate', 'CostVariable', 'CostFixed', 'CostInvest']:\n",
    "    query = \"DELETE FROM \" + table + \" WHERE tech IN\\\n",
    "    (SELECT tech FROM technologies WHERE sector='electric')\\\n",
    "    AND tech NOT IN ('\" + \"','\".join(techs_keep) + \"')\"\n",
    "    c.execute(query)\n",
    "\n",
    "#delete efficiencies from 2017 vintages\n",
    "for table in ['Efficiency','EmissionActivity']:\n",
    "    query = \"\"\"DELETE FROM \"\"\" + table + \"\"\" WHERE vintage=2017\"\"\"\n",
    "    c.execute(query)\n",
    "    \n",
    "        \n",
    "# Delete row from Efficiency if (t,v) retires at the begining of crent period (which is time_periods[i][0])\n",
    "c.execute(\"DELETE FROM Efficiency WHERE tech IN (SELECT tech FROM LifetimeProcess WHERE \\\n",
    "             LifetimeProcess.life_process+LifetimeProcess.vintage<=2020) \\\n",
    "             AND vintage IN (SELECT vintage FROM LifetimeProcess WHERE LifetimeProcess.life_process+\\\n",
    "             LifetimeProcess.vintage<=2020);\")\n",
    "\n",
    "# Delete row from Efficiency if (t,v) retires at the begining of crent period (which is time_periods[i][0])\n",
    "c.execute(\"DELETE FROM Efficiency WHERE tech IN (SELECT tech FROM LifetimeTech WHERE \\\n",
    "             LifetimeTech.life+Efficiency.vintage<=2020) AND \\\n",
    "             vintage NOT IN (SELECT vintage FROM LifetimeProcess WHERE LifetimeProcess.tech\\\n",
    "             =Efficiency.tech);\")\n",
    "\n",
    "# If row is not deleted via the last two DELETE commands, it might still be invalid for period\n",
    "#  time_periods[i][0] since they can have model default lifetime of 40 years. \n",
    "c.execute(\"DELETE FROM Efficiency WHERE tech IN (SELECT tech FROM Efficiency WHERE \\\n",
    "            40+Efficiency.vintage<=2020) AND \\\n",
    "            tech NOT IN (SELECT tech FROM LifetimeTech) AND \\\n",
    "            vintage NOT IN (SELECT vintage FROM LifetimeProcess WHERE LifetimeProcess.tech=Efficiency.tech);\")\n",
    "\n",
    "\n",
    "#remove demands, techinput split before 2020\n",
    "for table in ['Demand','TechInputSplit', 'MinActivity', 'MaxActivity','CostFixed','CostVariable']:\n",
    "    query = \"\"\"DELETE FROM \"\"\" + table +\"\"\" where periods < 2020\"\"\"\n",
    "    c.execute(query)\n",
    "for table in ['CostFixed','CostVariable']:\n",
    "    query = \"\"\"DELETE FROM \"\"\" + table +\"\"\" where vintage=2017\"\"\"\n",
    "    c.execute(query)\n",
    "for table in ['CostInvest', 'DiscountRate']:\n",
    "    query = \"\"\"DELETE FROM \"\"\" + table + \"\"\" where vintage < 2020\"\"\"\n",
    "    c.execute(query)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(outputdB, table_name, primary_key, foreign_key, key_val, references=0):\n",
    "    create_table_sql = \"\"\"CREATE TABLE IF NOT EXISTS \"\"\" + table_name + \"\"\" ( \n",
    "                         \"\"\" + primary_key + \"\"\" text primary key\"\"\"\n",
    "    \n",
    "    for key in key_val.keys():\n",
    "        create_table_sql += \"\"\", \"\"\" + key + \"\"\" \"\"\" + key_val[key] \n",
    "                         \n",
    "    if references!=0:\n",
    "        for key in references.keys():\n",
    "            create_table_sql += \"\"\", FOREIGN KEY(\"\"\" + foreign_key + \"\"\") REFERENCES \"\"\" + key + \"\"\"(\"\"\" + references[key] + \"\"\")\"\"\"\n",
    "            \n",
    "        create_table_sql += \"\"\");\"\"\"\n",
    "    else:\n",
    "        create_table_sql += \"\"\");\"\"\"\n",
    "    \n",
    "    conn = sqlite3.connect(outputdB)\n",
    "    c = conn.cursor()\n",
    "    c.execute(create_table_sql)\n",
    "    conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_mult_pkey(outputdB, table_name, primary_key, foreign_key, key_val, references=0):\n",
    "    create_table_sql = \"\"\"CREATE TABLE IF NOT EXISTS \"\"\" + table_name + \"\"\"(\"\"\"\n",
    "    \n",
    "    for key in key_val.keys():\n",
    "        create_table_sql +=  key + \"\"\" \"\"\" + key_val[key] + \"\"\", \"\"\"\n",
    "                         \n",
    "    if references!=0:\n",
    "        for key in references.keys():\n",
    "            create_table_sql += \"\"\" FOREIGN KEY(\"\"\" + foreign_key + \"\"\") REFERENCES \"\"\" + key + \"\"\"(\"\"\" + references[key] + \"\"\")\"\"\"\n",
    "            \n",
    "        #create_table_sql += \"\"\",\"\"\"\n",
    "    #else:\n",
    "        create_table_sql += \"\"\",\"\"\"\n",
    "        \n",
    "    create_table_sql+= \"\"\" PRIMARY KEY \"\"\" + primary_key  + \"\"\");\"\"\"\n",
    "    \n",
    "    conn = sqlite3.connect(outputdB)\n",
    "    c = conn.cursor()\n",
    "    c.execute(create_table_sql)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_regions(x):\n",
    "    return x.split('-')[1] + '-' + x.split('-')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def prep_tables(period, folder_name, gen_data, capfac_variability_data):\n",
    "    #PREPARE DATABASE TABLES\n",
    "    \n",
    "df_gen= all_gens_multi_year\n",
    "df_gen.rename(columns={'region':'regions'}, inplace=True)\n",
    "\n",
    "df_gen.loc[:,'regions'] = df_gen.loc[:,'regions'].str.replace('US_N','US')\n",
    "#df_gen.loc[:, 'tech'] =  df_gen.loc[:, 'Resource'] + '-' + df_gen.loc[:,'regions'] + '-' +df_gen.loc[:,'cluster'].map(int).map(str)\n",
    "\n",
    "#remove battery, pumped storage and ev_shifting (after column names have been assigned to capfac dataframe)\n",
    "df_gen = df_gen.loc[~df_gen.loc[:,'tech'].str.contains('E_LD_SHFT'),:]\n",
    "#df_gen = df_gen.loc[~df_gen.loc[:,'tech'].str.contains('battery'),:]\n",
    "#df_gen = df_gen.loc[~df_gen.loc[:,'tech'].str.contains('pumped'),:]\n",
    "#rename operating_year as vintage\n",
    "df_gen.rename(columns={'operating_year':'vintage'},inplace=True)\n",
    "df_gen= df_gen.astype({'vintage':int})\n",
    "df_gen.loc[:,'renewable_nonstor'] = df_gen.loc[:,'RPS'] | df_gen.loc[:,'HYDRO'] \n",
    "\n",
    "#capacityfactortech\n",
    "df_capfac= reduced_resource_profile.reset_index(drop=True)\n",
    "df_capfac.columns = all_gens.loc[:,'tech'] #rename columns to match df_gen technologies\n",
    "#remove battery and ev_shifting\n",
    "df_capfac.drop(columns=df_capfac.columns[df_capfac.columns.str.contains('E_LD_SHFT')], inplace=True)\n",
    "#df_capfac.drop(columns=df_capfac.columns[df_capfac.columns.str.contains('battery')], inplace=True)\n",
    "#df_capfac.drop(columns=df_capfac.columns[df_capfac.columns.str.contains('pumped')], inplace=True)\n",
    "\n",
    "intra_annual_periods = len(df_capfac)\n",
    "days = intra_annual_periods/24\n",
    "iter_val = 0\n",
    "for day in np.arange(days)+1:\n",
    "    df_capfac.loc[iter_val:iter_val+23,'season_name'] = 'S' + str(int(day))\n",
    "    df_capfac.loc[iter_val:iter_val+23,'time_of_day_name'] = ['H' + str(int(x)) for x in np.arange(1,25)]\n",
    "    iter_val+=24\n",
    "df_capfac = df_capfac.melt(id_vars=['season_name','time_of_day_name'])\n",
    "df_capfac = df_capfac.rename(columns={'variable':'tech', 'value':'cf_tech'})\n",
    "df_capfac.loc[:,'cf_tech_notes'] = 'from Power Genome: ' + scenario\n",
    "df_capfac.insert(0,'regions',return_region(df_capfac.loc[:,'tech']))\n",
    "#df_capfac.loc[:,'regions'] = df_capfac.loc[:,'regions'].str.replace('US_N','US')\n",
    "df_capfac.loc[:,'tech'] = return_tech(df_capfac.loc[:,'tech'])\n",
    "#remove techs that have constant capacity factor\n",
    "df_sum = df_capfac.groupby(by=['regions','tech']).sum().reset_index()\n",
    "df_fixed_capfac = df_sum[df_sum.cf_tech==intra_annual_periods]\n",
    "df_fixed_capfac.rename(columns={'cf_tech': 'cf_tech_max'}, inplace=True)\n",
    "df_capfac = df_capfac.merge(df_fixed_capfac, on = ['regions','tech'], how='outer')\n",
    "df_capfac = df_capfac[df_capfac.cf_tech_max!=intra_annual_periods]\n",
    "df_capfac = df_capfac.drop('cf_tech_max',axis=1)\n",
    "\n",
    "#segfrac for a chronological model has equal weights for all time periods, with a sum of 1\n",
    "df_segfrac = df_capfac.loc[:,['season_name','time_of_day_name']].copy()\n",
    "df_segfrac.drop_duplicates(inplace=True) \n",
    "df_segfrac.loc[:,'segfrac'] = 1/len(df_segfrac)\n",
    "df_segfrac.loc[:,'segfrac_notes'] = 'from Power Genome: ' + scenario\n",
    "\n",
    "#identify renewables that qualify for RPS and hydro, but not storage \n",
    "df_gen.loc[:,'renewable_nonstor'] = df_gen.loc[:,'RPS'] | df_gen.loc[:,'HYDRO'] \n",
    "\n",
    "#update lifetime as difference between retirement year and operating year\n",
    "df_gen.loc[:,'lifetime_diff'] = df_gen.loc[:,'retirement_year'] - df_gen.loc[:,'vintage']\n",
    "df_gen.loc[:,'lifetime_diff'].fillna(0, inplace=True)\n",
    "\n",
    "df_gen.loc[:,'lifetime'].fillna(0, inplace=True)\n",
    "df_gen.loc[:,'lifetime'] = df_gen.loc[:, ['lifetime', 'lifetime_diff']].max(axis=1)\n",
    "\n",
    "#capacitytoactivity for a chronological model, to normalize to annual time periods, assuming min value is hours\n",
    "df_c2a = df_gen.loc[:,['tech']].drop_duplicates()\n",
    "#remove battery and ev_shifting\n",
    "df_c2a = df_c2a.loc[~df_c2a.loc[:,'tech'].str.contains('ev_'),:]\n",
    "df_c2a = df_c2a.loc[~df_c2a.loc[:,'tech'].str.contains('battery'),:]\n",
    "df_c2a = df_c2a.loc[~df_c2a.loc[:,'tech'].str.contains('pumped'),:]\n",
    "\n",
    "df_c2a.loc[:,'c2a'] = 31.536 #8760/(days*24)\n",
    "df_c2a.loc[:,'c2a_notes'] = 'from Power Genome: ' + scenario\n",
    "df_c2a.insert(0,'regions',return_region(df_c2a.loc[:,'tech']))\n",
    "df_c2a.loc[:,'regions'] = df_c2a.loc[:,'regions'].str.replace('US_N','US')\n",
    "df_c2a.loc[:,'tech'] = return_tech(df_c2a.loc[:,'tech'])\n",
    "\n",
    "#investment, fixed and variable costs\n",
    "df_costs = df_gen.loc[:,['tech', 'capex', 'Fixed_OM_cost_per_MWyr', 'Var_OM_cost_per_MWh', 'vintage']].copy()\n",
    "df_costs.loc[:,'cost_invest'] = df_costs.loc[:, 'capex']/(10**3) #$/MW to #$M/GW\n",
    "df_costs.loc[:,'cost_fixed'] = df_costs.loc[:, 'Fixed_OM_cost_per_MWyr']*(10**3)/(10**6) #$/MWyr to #$M/GW-yr\n",
    "df_costs.loc[:,'cost_variable'] = df_costs.loc[:, 'Var_OM_cost_per_MWh']*(277777.78)/(10**6) #$/MWh to #$M/PJ\n",
    "df_costs.loc[:,'cost_invest_units'] = '$M/GW'\n",
    "df_costs.loc[:,'cost_invest_notes'] = 'from Power Genome: ' + scenario\n",
    "df_costs.loc[:,'cost_fixed_units'] = '$M/GWyr'\n",
    "df_costs.loc[:,'cost_fixed_notes'] = 'from Power Genome: ' + scenario\n",
    "df_costs.loc[:,'cost_variable_units'] = '$M/PJ'\n",
    "df_costs.loc[:,'cost_variable_notes'] = 'from Power Genome: ' + scenario\n",
    "df_costs.insert(0,'regions',return_region(df_costs.loc[:,'tech']))\n",
    "df_costs.loc[:,'tech'] = return_tech(df_costs.loc[:,'tech'])\n",
    "\n",
    "df_cost_invest = df_costs.loc[:,['regions','tech','vintage','cost_invest','cost_invest_units','cost_invest_notes']].copy()\n",
    "df_cost_invest = df_cost_invest.loc[df_cost_invest.cost_invest>0]\n",
    "\n",
    "df_costv = df_costs.loc[:,['regions','tech','vintage','cost_variable','cost_variable_units','cost_variable_notes']].drop_duplicates()\n",
    "df_cost_variable = pd.DataFrame(np.repeat(df_costv.values, len(all_periods), axis=0), columns= df_costv.columns)\n",
    "df_cost_variable.insert(1,'periods',int((len(df_cost_variable)/len(all_periods)))*all_periods)\n",
    "df_cost_variable = df_cost_variable[df_cost_variable.vintage<=df_cost_variable.periods]\n",
    "\n",
    "df_costf = df_costs.loc[:,['regions','tech','vintage','cost_fixed','cost_fixed_units','cost_fixed_notes']]\n",
    "df_cost_fixed = pd.DataFrame(np.repeat(df_costf.values, len(all_periods), axis=0), columns= df_costf.columns)\n",
    "df_cost_fixed.insert(1,'periods',int((len(df_cost_fixed)/len(all_periods)))*all_periods)\n",
    "df_cost_fixed = df_cost_fixed[df_cost_fixed.vintage<=df_cost_fixed.periods]\n",
    "\n",
    "#efficiency\n",
    "df_efficiency = df_gen.loc[:,['Resource','tech', 'Heat_rate_MMBTU_per_MWh','renewable_nonstor', 'vintage', 'flag', 'sector', 'tech_desc', 'tech_category']].copy()\n",
    "df_efficiency.rename(columns={'Resource':'input_comm'}, inplace=True)\n",
    "#convert technologies with no heat rate to 100% efficiency, by setting the heat rate to 3412.0/1000 MMBTU/MWh\n",
    "df_efficiency.loc[(df_efficiency.loc[:,'Heat_rate_MMBTU_per_MWh']==0),'Heat_rate_MMBTU_per_MWh'] = 3412.0/1000\n",
    "df_efficiency.loc[:, 'output_comm'] = df_efficiency.loc[:,'renewable_nonstor'].apply(lambda x: 'ELCP_Renewables' if x ==1 else 'ELCP')\n",
    "df_efficiency.loc[:,'efficiency'] = 3412.0/(df_efficiency.loc[:,'Heat_rate_MMBTU_per_MWh']*1000)\n",
    "df_efficiency.loc[:,'efficiency'].fillna(1.0, inplace=True)\n",
    "df_efficiency.drop(columns=['Heat_rate_MMBTU_per_MWh','renewable_nonstor'], inplace=True)\n",
    "df_efficiency.loc[:,'eff_notes'] = 'from Power Genome: ' + scenario\n",
    "#remove battery and ev_shifting\n",
    "df_efficiency = df_efficiency.loc[~df_efficiency.loc[:,'tech'].str.contains('ev_'),:]\n",
    "df_efficiency = df_efficiency.loc[~df_efficiency.loc[:,'tech'].str.contains('battery'),:]\n",
    "df_efficiency.insert(0,'regions',return_region(df_efficiency.loc[:,'tech']))\n",
    "df_efficiency.loc[:,'tech'] = return_tech(df_efficiency.loc[:,'tech'])\n",
    "\n",
    "#ramp up and down fractions\n",
    "df_ramp = df_gen.loc[(df_gen.loc[:,'Ramp_Up_percentage']>0) | (df_gen.loc[:,'Ramp_Dn_percentage']>0), ['regions','tech', 'Ramp_Up_percentage', 'Ramp_Dn_percentage']].copy()\n",
    "df_ramp.rename(columns={'Ramp_Up_percentage': 'ramp_up', 'Ramp_Dn_percentage': 'ramp_down'}, inplace=True)\n",
    "df_ramp.loc[:,'tech'] = return_tech(df_ramp.loc[:,'tech'])\n",
    "df_ramp = df_ramp[(df_ramp.ramp_up<1) | (df_ramp.ramp_down<1)]\n",
    "\n",
    "\n",
    "#lifetime\n",
    "df_lifetime = df_gen.loc[:,['tech','lifetime']].copy()\n",
    "df_lifetime.insert(0,'regions',return_region(df_lifetime.loc[:,'tech']))\n",
    "df_lifetime.loc[:,'tech'] = return_tech(df_lifetime.loc[:,'tech'])\n",
    "\n",
    "#existing capacity\n",
    "df_ex_cap = df_gen.loc[:, ['tech','Existing_Cap_MW', 'vintage' ]]\n",
    "df_ex_cap = df_ex_cap.loc[df_ex_cap.loc[:,'Existing_Cap_MW']>0,:]\n",
    "df_ex_cap.loc[:,'exist_cap'] = df_ex_cap.loc[:,'Existing_Cap_MW']/1000 #GW\n",
    "df_ex_cap.drop(columns=['Existing_Cap_MW'], inplace=True)\n",
    "df_ex_cap.loc[:,'exist_cap_units'] = 'GW'\n",
    "df_ex_cap.loc[:,'exist_cap_notes'] = 'from Power Genome: ' + scenario\n",
    "##FIX THIS for zero value years for EV load shifting, removing this row for now\n",
    "df_ex_cap = df_ex_cap.loc[df_ex_cap.loc[:,'vintage']!=0,:]\n",
    "df_ex_cap.insert(0,'regions',return_region(df_ex_cap.loc[:,'tech']))\n",
    "df_ex_cap.loc[:,'tech'] = return_tech(df_ex_cap.loc[:,'tech'])\n",
    "\n",
    "#discount rate\n",
    "df_wacc = df_gen.loc[(df_gen.loc[:,'waccnomtech']>0), ['tech', 'vintage', 'waccnomtech']].copy()\n",
    "df_wacc.rename(columns={'waccnomtech': 'tech_rate'}, inplace=True)\n",
    "df_wacc.loc[:,'tech_rate_notes'] = 'from Power Genome: ' + scenario\n",
    "df_wacc.insert(0,'regions',return_region(df_wacc.loc[:,'tech']))\n",
    "df_wacc.loc[:,'tech'] = return_tech(df_wacc.loc[:,'tech'])\n",
    "\n",
    "#capital recovery years\n",
    "df_cap_rec_years = df_gen.loc[(df_gen.loc[:,'cap_recovery_years']>0),['tech', 'cap_recovery_years']].copy()\n",
    "df_cap_rec_years.rename(columns={'cap_recovery_years': 'loan'}, inplace=True)\n",
    "df_cap_rec_years.loc[:,'loan_notes'] = 'from Power Genome: ' + scenario\n",
    "df_cap_rec_years.insert(0,'regions',return_region(df_cap_rec_years.loc[:,'tech']))\n",
    "df_cap_rec_years.loc[:,'tech'] = return_tech(df_cap_rec_years.loc[:,'tech'])\n",
    "\n",
    "#lifetime\n",
    "df_lifetime = df_gen.loc[(df_gen.loc[:,'lifetime']>0),['tech', 'lifetime']].copy()\n",
    "df_lifetime.rename(columns={'lifetime': 'life'}, inplace=True)\n",
    "df_lifetime.loc[:,'life_notes'] = 'from Power Genome: ' + scenario\n",
    "df_lifetime.insert(0,'regions',return_region(df_lifetime.loc[:,'tech']))\n",
    "df_lifetime.loc[:,'tech'] = return_tech(df_lifetime.loc[:,'tech'])\n",
    "\n",
    "#capacity credit\n",
    "df_ccredit = df_gen.loc[df_gen.loc[:,'CapRes']>0, ['tech', 'vintage', 'CapRes']].copy()\n",
    "df_ccredit  = pd.DataFrame(np.repeat(df_ccredit.values, len(all_periods), axis=0), columns= df_ccredit.columns)\n",
    "df_ccredit.insert(0,'periods',int((len(df_ccredit)/len(all_periods)))*all_periods)\n",
    "df_ccredit.rename(columns={'CapRes': 'cf_tech'}, inplace=True)\n",
    "df_ccredit.loc[:,'cf_tech_notes'] = 'from Power Genome: ' + scenario\n",
    "df_ccredit.insert(0,'regions',return_region(df_ccredit.loc[:,'tech']))\n",
    "df_ccredit.loc[:,'tech'] = return_tech(df_ccredit.loc[:,'tech'])\n",
    "\n",
    "#max capacity, for renewables, primarily\n",
    "df_maxcap = df_gen.loc[(df_gen.loc[:,'Max_Cap_MW']>0),['tech','Max_Cap_MW','vintage']].copy()\n",
    "df_maxcap.rename(columns={'Max_Cap_MW':'maxcap', 'vintage':'periods'}, inplace=True)\n",
    "df_maxcap.loc[:,'maxcap'] = df_maxcap.loc[:,'maxcap']/1000 #convert MW to GW\n",
    "df_maxcap.loc[:,'maxcap_units'] = 'GW' #convert MW to GW\n",
    "df_maxcap.loc[:,'maxcap_tech_notes'] = 'from Power Genome: ' + scenario\n",
    "df_maxcap.insert(0,'regions',return_region(df_maxcap.loc[:,'tech']))\n",
    "df_maxcap.loc[:,'tech'] = return_tech(df_maxcap.loc[:,'tech'])\n",
    "\n",
    "#load, estimating demand specific distribution, same distribution for all demands\n",
    "df_load_i = reduced_load_profile\n",
    "ind = list(reduced_load_profile.columns).index('Time_index')\n",
    "df_load_i = reduced_load_profile[reduced_load_profile.columns[ind+1:]]\n",
    "iter_val = 0\n",
    "for day in np.arange(days)+1:\n",
    "    df_load_i.loc[iter_val:iter_val+23,'season_name'] = 'S' + str(int(day))\n",
    "    df_load_i.loc[iter_val:iter_val+23,'time_of_day_name'] = ['H' + str(int(x)) for x in np.arange(1,25)]\n",
    "    iter_val+=24\n",
    "df_load_i = df_load_i.melt(id_vars=['season_name','time_of_day_name'])\n",
    "df_load_i = df_load_i.rename(columns={'variable':'regions', 'value':'dds_vals'})\n",
    "df_load_i.loc[:, 'regions'] = df_load_i.loc[:, 'regions'].str.replace('US_N','US')\n",
    "\n",
    "df_load = pd.DataFrame()\n",
    "for demn in ['RSC', 'RSH','RLT','ROELC','RWH','CLT','COELC','COEELC','CSC','CSH','CWH']:\n",
    "    for region in df_load_i.regions.unique():\n",
    "        df_load_i.loc[(df_load_i.regions==region),'demand_name'] = demn\n",
    "        df_load_i.loc[(df_load_i.regions==region),'dds'] = df_load_i.loc[(df_load_i.regions==region),'dds_vals']/df_load_i.loc[(df_load_i.regions==region),'dds_vals'].sum()\n",
    "        df_load = pd.concat([df_load, df_load_i.loc[(df_load_i.regions==region),['regions','season_name','time_of_day_name','demand_name','dds']]])\n",
    "df_load.loc[:,'dds_notes'] = 'from Power Genome: ' + scenario\n",
    "df_load.loc[:,'regions'] = df_load.loc[:,'regions'].str.replace('z1','US')\n",
    "\n",
    "if len(settings['region_aggregations'])>1:\n",
    "\n",
    "    #transmission efficiency\n",
    "    df_trans_efficiency = pd.DataFrame()\n",
    "    df_trans_efficiency.loc[:,'regions']  = transmission.loc[:,'Transmission Path Name'].str.replace('_to_','-')\n",
    "    df_trans_efficiency.loc[:,'input_comm'] = 'ELC'\n",
    "    df_trans_efficiency.loc[:,'tech'] = 'E_TRANS'\n",
    "    df_trans_efficiency.loc[:,'vintage'] = all_periods[0]-1\n",
    "    df_trans_efficiency.loc[:,'output_comm'] = 'ELC'\n",
    "    df_trans_efficiency.loc[:,'efficiency'] = 1 - transmission.loc[:,'Line_Loss_Percentage']\n",
    "    df_trans_efficiency.loc[:,'eff_notes'] = 'from Power Genome: ' + scenario\n",
    "    df_trans_efficiency.loc[:,'flag'] = 'p'\n",
    "    df_trans_efficiency.loc[:,'sector'] = 'electric'\n",
    "    df_trans_efficiency.loc[:,'tech_desc'] = '#electricity transmission'\n",
    "    \n",
    "    df_efficiency = pd.concat([df_efficiency, df_trans_efficiency])\n",
    "    df_trans_efficiency['regions'] = df_trans_efficiency['regions'].apply(swap_regions)\n",
    "    df_efficiency = pd.concat([df_efficiency, df_trans_efficiency])\n",
    "\n",
    "    #transmission costs\n",
    "    df_trans_c = pd.DataFrame()\n",
    "    df_trans_c.loc[:,'regions']  = transmission.loc[:,'Transmission Path Name'].str.replace('_to_','-')\n",
    "    df_trans_c.loc[:,'tech'] = 'E_TRANS'\n",
    "    df_trans_c.loc[:,'vintage'] = all_periods[0]-1\n",
    "    df_trans_c.loc[:, 'cost_fixed'] = transmission.loc[:, 'Line_Reinforcement_Cost_per_MW_yr']*(10**3)/(10**6) #$/MWyr to #$M/GW-yr\n",
    "    df_trans_c.loc[:, 'cost_fixed_units'] = '$M/GWyr'\n",
    "    df_trans_c.loc[:, 'cost_fixed_notes'] = 'from Power Genome: ' + scenario\n",
    "    df_trans_costs = pd.DataFrame(np.repeat(df_trans_c.values, len(all_periods), axis=0), columns= df_trans_c.columns)\n",
    "    df_trans_costs.insert(1,'periods',int((len(df_trans_costs)/len(all_periods)))*all_periods)\n",
    "    df_trans_costs = df_trans_costs[df_trans_costs.vintage<=df_trans_costs.periods]\n",
    "    \n",
    "    df_cost_fixed = pd.concat([df_cost_fixed, df_trans_costs])\n",
    "    df_trans_costs['regions'] = df_trans_costs['regions'].apply(swap_regions)\n",
    "    df_cost_fixed = pd.concat([df_cost_fixed, df_trans_costs])\n",
    "\n",
    "    #trans maximum capacity\n",
    "    df_trans_cap = pd.DataFrame()\n",
    "\n",
    "    df_trans_cap.loc[:,'regions']  = transmission.loc[:,'Transmission Path Name'].str.replace('_to_','-')\n",
    "    df_trans_cap.loc[:,'vintage']  = all_periods[0]-1\n",
    "\n",
    "    df_trans_cap.loc[:,'exist_cap'] = abs(transmission.loc[:,['Line_Max_Flow_MW', 'Line_Min_Flow_MW']]).max(axis=1).values/1000\n",
    "\n",
    "    df_trans_cap.insert(1,'tech', 'E_TRANS')\n",
    "    df_trans_cap.loc[:,'exist_cap_units'] = 'GW'\n",
    "    df_trans_cap.loc[:,'exist_cap_notes'] = 'from Power Genome: ' + scenario\n",
    "    \n",
    "    df_ex_cap = pd.concat([df_ex_cap, df_trans_cap])\n",
    "    df_trans_cap['regions'] = df_trans_cap['regions'].apply(swap_regions)\n",
    "    df_ex_cap = pd.concat([df_ex_cap, df_trans_cap])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add input_comm from commodities list\n",
    "df_efficiency.loc[df_efficiency.loc[:,'input_comm'].str.contains('BIO'),'input_comm'] = 'AGR'\n",
    "df_efficiency.loc[df_efficiency.loc[:,'input_comm'].str.contains('SOL'),'input_comm'] = 'ethos_R'\n",
    "df_efficiency.loc[df_efficiency.loc[:,'input_comm'].str.contains('WND'),'input_comm'] = 'ethos_R'\n",
    "df_efficiency.loc[df_efficiency.loc[:,'input_comm'].str.contains('HYD'),'input_comm'] = 'ethos_R'\n",
    "df_efficiency.loc[df_efficiency.loc[:,'input_comm'].str.contains('GEO'),'input_comm'] = 'ethos_R'\n",
    "df_efficiency.loc[df_efficiency.loc[:,'input_comm'].str.contains('URN'),'input_comm'] = 'URN_N'\n",
    "df_efficiency.loc[df_efficiency.loc[:,'input_comm'].str.contains('NG'),'input_comm'] = 'E_NGA'\n",
    "df_efficiency.loc[df_efficiency.loc[:,'input_comm'].str.contains('COAL'),'input_comm'] = 'COALSTM_N'\n",
    "df_efficiency.loc[df_efficiency.loc[:,'input_comm'].str.contains('E_Batt'),'input_comm'] = 'ELCP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change existing wind and solar efficiencies to 1\n",
    "df_efficiency.loc[(df_efficiency.loc[:,'tech'].str.contains('wind')) & (df_efficiency.loc[:,'vintage']<start_year), 'efficiency'] = 1\n",
    "df_efficiency.loc[(df_efficiency.loc[:,'tech'].str.contains('solar')) & (df_efficiency.loc[:,'vintage']<start_year), 'efficiency'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRITE TO DATABASE\n",
    "\n",
    "\n",
    "#CapacityToActivity\n",
    "df_table = df_c2a.drop_duplicates()\n",
    "sqlite_table = 'CapacityToActivity'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#SegFrac\n",
    "df_table = df_segfrac.drop_duplicates()\n",
    "sqlite_table = 'SegFrac'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#CostInvest\n",
    "df_table = df_cost_invest.copy()\n",
    "df_table = df_table.loc[df_table.loc[:,'cost_invest']>0,:]\n",
    "sqlite_table = 'CostInvest'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#CostVariable\n",
    "df_table = df_cost_variable.drop_duplicates()\n",
    "sqlite_table = 'CostVariable'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#CostFixed\n",
    "df_table = df_cost_fixed.drop_duplicates()\n",
    "sqlite_table = 'CostFixed'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#Efficiency\n",
    "df_table = df_efficiency[['regions', 'input_comm','tech','vintage','output_comm', 'efficiency', 'eff_notes']].drop_duplicates()\n",
    "sqlite_table = 'Efficiency'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#tech_curtailment\n",
    "df_table = pd.DataFrame(df_efficiency.loc[['wind' in x or 'pv' in x or 'solar' in x for x in df_efficiency.loc[:,'tech']],'tech'].drop_duplicates())\n",
    "df_table.insert(1,'notes','')\n",
    "sqlite_table = 'tech_curtailment'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#tech_nonrenewable\n",
    "df_table = df_efficiency.loc[df_efficiency.loc[:,'output_comm']=='ELCP','tech'].drop_duplicates()\n",
    "sqlite_table = 'tech_nonrenewable'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#tech_renewable\n",
    "df_table = df_efficiency.loc[df_efficiency.loc[:,'output_comm']=='ELCP_Renewables','tech'].drop_duplicates()\n",
    "sqlite_table = 'tech_renewable'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#tech_production in technologies table\n",
    "df_table = df_efficiency[['tech', 'flag', 'sector', 'tech_desc', 'tech_category']].drop_duplicates()\n",
    "df_table = df_table[~df_table['tech'].isin(tech_table['tech_technologies'])]\n",
    "sqlite_table = 'technologies'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#Ramping\n",
    "#ramping techs\n",
    "references = dict()\n",
    "references['technologies'] = 'tech'\n",
    "create_table(outputdB, 'tech_ramping', 'tech', 'tech', dict(),references)\n",
    "\n",
    "df_table = df_ramp.loc[:,['tech']].drop_duplicates()\n",
    "sqlite_table = 'tech_ramping'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#ramp up\n",
    "ramp_up_pairs = dict()\n",
    "ramp_up_pairs['regions'] = 'text'\n",
    "ramp_up_pairs['tech'] = 'text'\n",
    "ramp_up_pairs['ramp_up'] = 'real'\n",
    "references = dict()\n",
    "references['technologies'] = 'tech'\n",
    "create_table_mult_pkey(outputdB, 'RampUp', \"(regions, tech)\", 'tech', ramp_up_pairs,references)\n",
    "\n",
    "df_table = df_ramp.loc[:,['regions','tech', 'ramp_up']].drop_duplicates()\n",
    "sqlite_table = 'RampUp'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#ramp down\n",
    "ramp_up_pairs = dict()\n",
    "ramp_up_pairs['regions'] = 'text'\n",
    "ramp_up_pairs['tech'] = 'text'\n",
    "ramp_up_pairs['ramp_up'] = 'real'\n",
    "references = dict()\n",
    "references['technologies'] = 'tech'\n",
    "create_table_mult_pkey(outputdB, 'RampDown', \"(regions, tech)\", 'tech', ramp_up_pairs,references)\n",
    "\n",
    "df_table = df_ramp.loc[:,['regions','tech', 'ramp_down']].drop_duplicates()\n",
    "sqlite_table = 'RampDown'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#ExistingCapacity\n",
    "df_table = df_ex_cap.drop_duplicates()\n",
    "sqlite_table = 'ExistingCapacity'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#DiscountRate\n",
    "df_table = df_wacc.loc[:,['regions','tech', 'vintage', 'tech_rate', 'tech_rate_notes']].drop_duplicates()\n",
    "sqlite_table = 'DiscountRate'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#LifetimeLoanTech\n",
    "df_table = df_cap_rec_years.drop_duplicates()\n",
    "sqlite_table = 'LifetimeLoanTech'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#LifetimeTech\n",
    "conn = sqlite3.connect(outputdB)\n",
    "c = conn.cursor()\n",
    "df_lifetime_current = pd.read_sql_query(\"SELECT * FROM LifetimeTech\", conn)\n",
    "nonoverlap_tech = df_lifetime_current[~df_lifetime_current['tech'].isin(df_lifetime['tech'])]\n",
    "query = \"DELETE FROM LifetimeTech\"\n",
    "c.execute(query)\n",
    "conn.commit()\n",
    "conn.close()\n",
    "df_lifetime.loc[:,'life']=120 #all to 120 years for now\n",
    "df_table = pd.concat([nonoverlap_tech, df_lifetime.drop_duplicates()])\n",
    "sqlite_table = 'LifetimeTech'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#CapacityCredit\n",
    "df_table = df_ccredit.loc[:,['regions','periods','tech', 'vintage','cf_tech','cf_tech_notes']].drop_duplicates()\n",
    "sqlite_table = 'CapacityCredit'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#tech_reserve\n",
    "df_table = df_ccredit.loc[:,['tech','cf_tech_notes']].drop_duplicates(subset='tech')\n",
    "df_table = df_table[~df_table['tech'].isin(tech_table['tech_technologies'])]\n",
    "df_table.rename(columns={'cf_tech_notes':'notes'}, inplace=True)\n",
    "sqlite_table = 'tech_reserve'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#CapacityFactorTech\n",
    "df_table = df_capfac.drop_duplicates()\n",
    "sqlite_table = 'CapacityFactorTech'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#MaxCapacity\n",
    "df_table = df_maxcap.loc[:,['regions','periods','tech', 'maxcap','maxcap_units','maxcap_tech_notes']].drop_duplicates()\n",
    "sqlite_table = 'MaxCapacity'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#DSD\n",
    "df_table = df_load.loc[:,['regions','season_name','time_of_day_name','demand_name','dds','dds_notes']]\n",
    "sqlite_table = 'DemandSpecificDistribution'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#time_season\n",
    "df_table = pd.DataFrame(df_capfac.loc[:,'season_name'].drop_duplicates())\n",
    "df_table.rename(columns={'season_name':'t_season'},inplace=True)\n",
    "sqlite_table = 'time_season'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#time_of_day\n",
    "df_table = pd.DataFrame(df_capfac.loc[:,'time_of_day_name'].drop_duplicates())\n",
    "df_table.rename(columns={'time_of_day_name':'t_day'},inplace=True)\n",
    "sqlite_table = 'time_of_day'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#time_periods\n",
    "df_table = pd.DataFrame(columns=['t_periods','flag'])\n",
    "#df_table.loc[:,'t_periods'] = np.sort(df_costs.loc[:,'vintage'].unique().astype(int))\n",
    "df_table.loc[:,'t_periods'] = np.hstack((np.arange(df_costs.loc[:,'vintage'].astype(int).min()-1,2020), \n",
    "                                         np.arange(2020,2056,5)))\n",
    "df_table.loc[:,'flag'] = ['e' if x <2020  else 'f' for x in df_table.loc[:,'t_periods']]\n",
    "sqlite_table = 'time_periods'\n",
    "write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "\n",
    "if len(settings['region_aggregations'])>1 & ('E_TRANS' in df_efficiency.tech.unique()):\n",
    "    df_table = pd.DataFrame(columns = ['tech', 'notes'])\n",
    "    df_table.loc[0] = ['E_TRANS','']\n",
    "    sqlite_table = 'tech_exchange'\n",
    "    write_sql(df_table, sqlite_table, outputdB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map regions to descriptions\n",
    "map_region_desc = dict()\n",
    "map_region_desc['CA'] = 'California'\n",
    "map_region_desc['NW'] = 'Northwestern US'\n",
    "map_region_desc['SW'] =  'Southwestern US'\n",
    "map_region_desc['TX'] =  'Texas'\n",
    "map_region_desc['N_CEN'] =  'North Central US'\n",
    "map_region_desc['CEN'] =  'Central US'\n",
    "map_region_desc['SE'] =  'Southeastern US'\n",
    "map_region_desc['MID_AT'] =  'Mid Atlantic US'\n",
    "map_region_desc['NE'] = 'Northeastern US'\n",
    "\n",
    "df_regions = pd.DataFrame.from_dict(map_region_desc, orient='index').reset_index()\n",
    "df_regions.columns = ['regions', 'region_note']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(outputdB)\n",
    "c = conn.cursor()\n",
    "\n",
    "iterval = 0\n",
    "while len(c.execute(\"SELECT * FROM Efficiency WHERE output_comm NOT IN (SELECT input_comm FROM Efficiency)\\\n",
    "                       AND output_comm NOT IN (SELECT comm_name FROM commodities WHERE flag='d');\").fetchall()) > 0:\n",
    "\n",
    "    c.execute(\"DELETE FROM Efficiency WHERE output_comm NOT IN (SELECT input_comm FROM Efficiency) \\\n",
    "             AND output_comm NOT IN (SELECT comm_name FROM commodities WHERE flag='d');\")\n",
    "    iterval+=1\n",
    "    if iterval>10:\n",
    "        break\n",
    "print(iterval)\n",
    "\n",
    "for table in table_list:\n",
    "    if table[0] != 'Efficiency':  \n",
    "        df_cols = c.execute(\"SELECT * FROM pragma_table_info('\" + table[0] + \"')\").fetchall()\n",
    "        df_cols = [x[1] for x in df_cols]\n",
    "        if 'tech' in df_cols:\n",
    "            c.execute(\"UPDATE \"+str(table[0])+\" SET tech = TRIM(tech, CHAR(37,13,10));\")\n",
    "            try:\n",
    "                if 'vintage' in df_cols:\n",
    "                    # If t doesn't exist in Efficiency table after the deletions made above, \n",
    "                    # it is deleted from other tables.\n",
    "                    c.execute(\"DELETE FROM \"+str(table[0])+\" WHERE tech || vintage \\\n",
    "                        NOT IN (SELECT tech || vintage FROM Efficiency)\")\n",
    "                else:\n",
    "                    c.execute(\"DELETE FROM \"+str(table[0])+\" WHERE tech NOT IN (SELECT tech FROM Efficiency);\")\n",
    "            except:\n",
    "                print(table[0])\n",
    "        if table[0] == 'EmissionActivity':\n",
    "            c.execute(\"DELETE FROM EmissionActivity WHERE input_comm || tech || vintage || output_comm \\\n",
    "                        NOT IN (SELECT input_comm || tech || vintage || output_comm FROM Efficiency)\")\n",
    "\n",
    "\n",
    "c.execute(\"UPDATE commodities SET comm_name = TRIM(comm_name, CHAR(10,13,37))\")\n",
    "    # delete unused commodities otherwise the model throws an error\n",
    "c.execute(\"DELETE FROM commodities WHERE flag!='e' AND comm_name NOT IN (SELECT input_comm from Efficiency UNION SELECT output_comm from Efficiency);\")\n",
    "\n",
    "#update regional transportation demands \n",
    "df_transport = pd.read_csv('../TransportationSector/NREL_EFS/transportation_regional_demands_adjusted.csv')\n",
    "df_transport = df_transport[['regions', 'periods', 'demand_comm', 'demand','demand_units','demand_notes']]\n",
    "query = \"DELETE FROM Demand WHERE demand_units LIKE '%vmt%' \\\n",
    "OR demand_units LIKE '%mile%' OR demand_units LIKE '%bpm%'OR demand_units LIKE '%btm%'\"\n",
    "c.execute(query)\n",
    "df_transport.to_sql('Demand',conn, if_exists='append', index=False)\n",
    "\n",
    "#update regional building demands \n",
    "df_buildings = pd.read_csv('../BuildingsSector/NREL_EFS/buildings_regional_demands_adjusted.csv')\n",
    "df_buildings = df_buildings[['regions', 'periods', 'demand_comm', 'demand','demand_units','demand_notes']]\n",
    "query = \"DELETE FROM Demand WHERE \\\n",
    "substr(demand_comm,1,1) = 'C'OR substr(demand_comm,1,1) = 'R'\"\n",
    "c.execute(query)\n",
    "df_buildings.to_sql('Demand',conn, if_exists='append', index=False)\n",
    "\n",
    "#update regional industry demands \n",
    "df_industry = pd.read_csv('../IndustrialSector/NREL_EFS/industry_regional_demands_adjusted.csv')\n",
    "df_industry = df_industry[['regions', 'periods', 'demand_comm', 'demand','demand_units','demand_notes']]\n",
    "query = \"DELETE FROM Demand  \\\n",
    "WHERE demand_comm LIKE '%IND%'\"\n",
    "c.execute(query)\n",
    "df_industry.to_sql('Demand',conn, if_exists='append', index=False)\n",
    "\n",
    "#find remaining demands at US level, divide by 9 to get demands in each region\n",
    "df_remains = pd.read_sql_query(\"SELECT * FROM Demand WHERE regions='US'\", conn)\n",
    "query = \"DELETE FROM Demand WHERE regions='US'\"\n",
    "c.execute(query)\n",
    "for reg in df_industry.regions.unique():\n",
    "    df_remains_new = df_remains.copy()\n",
    "    df_remains_new.regions=reg\n",
    "    df_remains_new['demand'] /=9\n",
    "    df_remains_new.to_sql('Demand',conn, if_exists='append', index=False)\n",
    "\n",
    "for table in table_list:\n",
    "    df_cols = c.execute(\"SELECT * FROM pragma_table_info('\" + table[0] + \"')\").fetchall()\n",
    "    df_cols = [x[1] for x in df_cols]\n",
    "    \n",
    "    if table[0]=='regions':\n",
    "        query = \"DELETE FROM \" + table[0]\n",
    "        c.execute(query)\n",
    "        df_regions.to_sql(table[0],conn, if_exists='append', index=False)\n",
    "    elif 'regions' in df_cols:\n",
    "        df_remains = pd.read_sql_query(\"SELECT * FROM \" + table[0] + \" WHERE regions='US'\", conn)\n",
    "        query = \"DELETE FROM \" + table[0] + \" WHERE regions='US'\"\n",
    "        c.execute(query)\n",
    "        for reg in df_industry.regions.unique():\n",
    "            df_remains_new = df_remains.copy()\n",
    "            df_remains_new.regions=reg\n",
    "            if table[0] in ['ExistingCapacity']:\n",
    "                df_remains_new['exist_cap'] /=9\n",
    "            #if table[0] in ['GrowthRateMax']:\n",
    "            #    df_remains_new['growthrate_max'] /=9\n",
    "            #if table[0] in ['GrowthRateSeed']:\n",
    "            #    df_remains_new['growthrate_seed'] /=9\n",
    "            if table[0] in ['MinActivity']:\n",
    "                df_remains_new['minact'] /=20\n",
    "            df_remains_new.to_sql(table[0],conn, if_exists='append', index=False)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con=sqlite3.connect(outputdB, isolation_level=None)\n",
    "con.execute(\"VACUUM\")\n",
    "con.commit()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(outputdB)\n",
    "with open(outputdB.replace('ite',''), 'w') as f:\n",
    "    for line in conn.iterdump():\n",
    "        '''\n",
    "        if 'e+' in line or 'e-0' in line:\n",
    "            val = [x for x in line.split(',') if 'e+' in x or 'e-0' in x][0]\n",
    "            ls = line.split(',')\n",
    "            ind = ls.index(val)\n",
    "            ls[ind] = [str(round(float(x),2)) for x in line.split(',') if 'e+' in x or 'e-0' in x ][0]\n",
    "            line = ','.join(ls)\n",
    "        '''\n",
    "        if 'CREATE TABLE' in line:\n",
    "            f.write('\\n' )\n",
    "        f.write('%s\\n' % line)\n",
    "        \n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
